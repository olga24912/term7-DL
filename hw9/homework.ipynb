{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework:\n",
    "# Deep Convolutional Generative Adversarial Network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of an implementation of DCGAN can be found in [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/abs/1511.06434)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the libraries we will need.\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.contrib.slim as slim\n",
    "import os\n",
    "import scipy.misc\n",
    "import scipy\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the MNIST dataset. input_data is a library that downloads the dataset and uzips it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist =  input_data.read_data_sets('data/fashion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function performns a leaky relu activation, which is needed for the discriminator network.\n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "    \n",
    "#The below functions are taken from carpdem20's implementation https://github.com/carpedm20/DCGAN-tensorflow\n",
    "#They allow for saving sample images from the generator to follow progress\n",
    "def save_images(images, size, image_path):\n",
    "    return imsave(inverse_transform(images), size, image_path)\n",
    "\n",
    "def imsave(images, size, path):\n",
    "    return scipy.misc.imsave(path, merge(images, size))\n",
    "\n",
    "def inverse_transform(images):\n",
    "    return (images+1.)/2.\n",
    "\n",
    "def merge(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((h * size[0], w * size[1]))\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % size[1]\n",
    "        j = idx // size[1]\n",
    "        img[j*h:j*h+h, i*w:i*w+w] = image\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    zP = slim.fully_connected(z,4*4*256,normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_project',weights_initializer=initializer)\n",
    "    zCon = tf.reshape(zP,[-1,4,4,256])\n",
    "    \n",
    "    gen1 = slim.convolution2d_transpose(\\\n",
    "        zCon,num_outputs=64,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv1', weights_initializer=initializer)\n",
    "    \n",
    "    gen2 = slim.convolution2d_transpose(\\\n",
    "        gen1,num_outputs=32,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv2', weights_initializer=initializer)\n",
    "    \n",
    "    gen3 = slim.convolution2d_transpose(\\\n",
    "        gen2,num_outputs=16,kernel_size=[5,5],stride=[2,2],\\\n",
    "        padding=\"SAME\",normalizer_fn=slim.batch_norm,\\\n",
    "        activation_fn=tf.nn.relu,scope='g_conv3', weights_initializer=initializer)\n",
    "    \n",
    "    g_out = slim.convolution2d_transpose(\\\n",
    "        gen3,num_outputs=1,kernel_size=[32,32],padding=\"SAME\",\\\n",
    "        biases_initializer=None,activation_fn=tf.nn.tanh,\\\n",
    "        scope='g_out', weights_initializer=initializer)\n",
    "    \n",
    "    return g_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1 (13 points)\n",
    "Fill parameter for the discrimiator architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(bottom, reuse=False):\n",
    "    with slim.arg_scope([slim.fully_connected, slim.convolution2d], reuse=reuse, weights_initializer=initializer):\n",
    "        with slim.arg_scope([slim.convolution2d], kernel_size=[5, 5], stride=[2, 2], padding=\"SAME\", activation_fn=lrelu):\n",
    "            dis1 = slim.convolution2d(bottom, scope='d_conv1', num_outputs=16)\n",
    "\n",
    "            dis2 = slim.convolution2d(dis1, scope='d_conv2', num_outputs=32, normalizer_fn=slim.batch_norm)\n",
    "\n",
    "            dis3 = slim.convolution2d(dis2, scope='d_conv3', num_outputs=64, normalizer_fn=slim.batch_norm)\n",
    "        \n",
    "            dis4 = slim.convolution2d(dis3, scope='d_conv4', num_outputs=128, normalizer_fn=slim.batch_norm)\n",
    "        \n",
    "            d_out = slim.fully_connected(slim.flatten(dis4), scope='d_out', num_outputs=1, activation_fn=tf.nn.sigmoid)\n",
    "                                     \n",
    "    return d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "z_size = 100 #Size of z vector used for generator.\n",
    "\n",
    "#This initializaer is used to initialize all the weights of the network.\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02)\n",
    "\n",
    "#These two placeholders are used for input into the generator and discriminator, respectively.\n",
    "z_in = tf.placeholder(shape=[None,z_size],dtype=tf.float32) #Random vector\n",
    "real_in = tf.placeholder(shape=[None,32,32,1],dtype=tf.float32) #Real images\n",
    "Gz = generator(z_in) #Generates images from random z vectors\n",
    "Dx = discriminator(real_in) #Produces probabilities for real images\n",
    "Dg = discriminator(Gz,reuse=True) #Produces probabilities for generator images\n",
    "\n",
    "\n",
    "#These functions together define the optimization objective of the GAN.\n",
    "d_loss = -tf.reduce_mean(tf.log(Dx) + tf.log(1.-Dg)) #This optimizes the discriminator.\n",
    "g_loss = -tf.reduce_mean(tf.log(Dg)) #This optimizes the generator.\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "\n",
    "#The below code is responsible for applying gradient descent to update the GAN.\n",
    "trainerD = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "trainerG = tf.train.AdamOptimizer(learning_rate=0.0002,beta1=0.5)\n",
    "d_grads = trainerD.compute_gradients(d_loss,tvars[9:]) #Only update the weights for the discriminator network.\n",
    "g_grads = trainerG.compute_gradients(g_loss,tvars[0:9]) #Only update the weights for the generator network.\n",
    "\n",
    "update_D = trainerD.apply_gradients(d_grads)\n",
    "update_G = trainerG.apply_gradients(g_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training the network\n",
    "I strongly advise you to skip this cell and go the the next one since training will take you enormous amount of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.494923 Disc Loss: 1.58332\n",
      "Gen Loss: 0.978343 Disc Loss: 1.21681\n",
      "Gen Loss: 1.04484 Disc Loss: 1.23895\n",
      "Gen Loss: 1.11505 Disc Loss: 1.27017\n",
      "Gen Loss: 1.07836 Disc Loss: 1.27919\n",
      "Gen Loss: 0.980969 Disc Loss: 1.30305\n",
      "Gen Loss: 0.848795 Disc Loss: 1.27014\n",
      "Gen Loss: 0.868419 Disc Loss: 1.20503\n",
      "Gen Loss: 0.868412 Disc Loss: 1.39678\n",
      "Gen Loss: 0.729578 Disc Loss: 1.39742\n",
      "Gen Loss: 0.725088 Disc Loss: 1.37276\n",
      "Gen Loss: 0.968822 Disc Loss: 1.26048\n",
      "Gen Loss: 0.765627 Disc Loss: 1.28474\n",
      "Gen Loss: 0.791621 Disc Loss: 1.16306\n",
      "Gen Loss: 0.927308 Disc Loss: 1.19544\n",
      "Gen Loss: 0.799412 Disc Loss: 1.15373\n",
      "Gen Loss: 1.05964 Disc Loss: 1.19071\n",
      "Gen Loss: 0.928783 Disc Loss: 1.05031\n",
      "Gen Loss: 1.74318 Disc Loss: 1.20336\n",
      "Gen Loss: 0.954943 Disc Loss: 1.29172\n",
      "Gen Loss: 0.84365 Disc Loss: 1.20486\n",
      "Gen Loss: 0.578524 Disc Loss: 1.12708\n",
      "Gen Loss: 1.2152 Disc Loss: 1.3482\n",
      "Gen Loss: 1.0692 Disc Loss: 1.40164\n",
      "Gen Loss: 0.777747 Disc Loss: 1.27026\n",
      "Gen Loss: 0.943422 Disc Loss: 1.23209\n",
      "Gen Loss: 0.868528 Disc Loss: 1.07254\n",
      "Gen Loss: 0.968872 Disc Loss: 1.19904\n",
      "Gen Loss: 1.1181 Disc Loss: 1.10648\n",
      "Gen Loss: 1.03944 Disc Loss: 1.09815\n",
      "Gen Loss: 0.942399 Disc Loss: 1.22772\n",
      "Gen Loss: 1.06882 Disc Loss: 1.05816\n",
      "Gen Loss: 0.874393 Disc Loss: 0.97222\n",
      "Gen Loss: 0.762419 Disc Loss: 1.12617\n",
      "Gen Loss: 0.704325 Disc Loss: 1.13865\n",
      "Gen Loss: 0.715094 Disc Loss: 1.04506\n",
      "Gen Loss: 0.991712 Disc Loss: 0.940467\n",
      "Gen Loss: 0.883567 Disc Loss: 1.09423\n",
      "Gen Loss: 1.24943 Disc Loss: 0.891838\n",
      "Gen Loss: 1.32049 Disc Loss: 1.03729\n",
      "Gen Loss: 1.23102 Disc Loss: 1.04654\n",
      "Gen Loss: 0.710968 Disc Loss: 1.00228\n",
      "Gen Loss: 0.545628 Disc Loss: 1.03686\n",
      "Gen Loss: 1.23766 Disc Loss: 1.07521\n",
      "Gen Loss: 0.984989 Disc Loss: 1.85708\n",
      "Gen Loss: 1.3007 Disc Loss: 0.966335\n",
      "Gen Loss: 0.991786 Disc Loss: 1.48249\n",
      "Gen Loss: 1.11543 Disc Loss: 0.919468\n",
      "Gen Loss: 0.915129 Disc Loss: 1.28376\n",
      "Gen Loss: 1.26846 Disc Loss: 1.15821\n",
      "Gen Loss: 1.34585 Disc Loss: 0.885675\n",
      "Gen Loss: 1.14916 Disc Loss: 1.10818\n",
      "Gen Loss: 0.932241 Disc Loss: 0.890979\n",
      "Gen Loss: 1.50314 Disc Loss: 1.04442\n",
      "Gen Loss: 1.20399 Disc Loss: 1.255\n",
      "Gen Loss: 0.813103 Disc Loss: 0.97108\n",
      "Gen Loss: 1.39473 Disc Loss: 0.9358\n",
      "Gen Loss: 1.41334 Disc Loss: 0.911555\n",
      "Gen Loss: 0.840732 Disc Loss: 1.1695\n",
      "Gen Loss: 1.37646 Disc Loss: 0.855551\n",
      "Gen Loss: 1.163 Disc Loss: 0.858841\n",
      "Gen Loss: 0.859631 Disc Loss: 0.963507\n",
      "Gen Loss: 1.11241 Disc Loss: 0.900115\n",
      "Gen Loss: 1.76739 Disc Loss: 0.883357\n",
      "Gen Loss: 1.25944 Disc Loss: 0.808287\n",
      "Gen Loss: 1.10087 Disc Loss: 1.0084\n",
      "Gen Loss: 1.09282 Disc Loss: 0.907207\n",
      "Gen Loss: 0.428331 Disc Loss: 1.01936\n",
      "Gen Loss: 1.24687 Disc Loss: 0.863267\n",
      "Gen Loss: 1.19546 Disc Loss: 0.821472\n",
      "Gen Loss: 0.628319 Disc Loss: 0.941739\n",
      "Gen Loss: 0.819299 Disc Loss: 0.975161\n",
      "Gen Loss: 1.12488 Disc Loss: 1.1011\n",
      "Gen Loss: 0.588491 Disc Loss: 1.05768\n",
      "Gen Loss: 0.812633 Disc Loss: 1.06304\n",
      "Gen Loss: 1.46055 Disc Loss: 1.22176\n",
      "Gen Loss: 2.13722 Disc Loss: 1.14408\n",
      "Gen Loss: 0.92893 Disc Loss: 0.901329\n",
      "Gen Loss: 0.775551 Disc Loss: 1.21797\n",
      "Gen Loss: 0.708335 Disc Loss: 1.28796\n",
      "Gen Loss: 1.05877 Disc Loss: 0.955917\n",
      "Gen Loss: 0.940648 Disc Loss: 0.881376\n",
      "Gen Loss: 0.94317 Disc Loss: 1.30041\n",
      "Gen Loss: 0.978028 Disc Loss: 0.876876\n",
      "Gen Loss: 0.81212 Disc Loss: 0.903421\n",
      "Gen Loss: 1.36205 Disc Loss: 1.21596\n",
      "Gen Loss: 1.32854 Disc Loss: 0.786101\n",
      "Gen Loss: 0.869071 Disc Loss: 0.914201\n",
      "Gen Loss: 0.926719 Disc Loss: 1.14974\n",
      "Gen Loss: 0.854108 Disc Loss: 0.96413\n",
      "Gen Loss: 0.671308 Disc Loss: 1.29648\n",
      "Gen Loss: 1.76237 Disc Loss: 1.03465\n",
      "Gen Loss: 1.45859 Disc Loss: 0.899554\n",
      "Gen Loss: 0.880219 Disc Loss: 0.955109\n",
      "Gen Loss: 1.32329 Disc Loss: 1.03411\n",
      "Gen Loss: 0.947292 Disc Loss: 0.834776\n",
      "Gen Loss: 0.629633 Disc Loss: 1.25565\n",
      "Gen Loss: 0.857224 Disc Loss: 0.952337\n",
      "Gen Loss: 1.17071 Disc Loss: 0.954512\n",
      "Gen Loss: 0.467248 Disc Loss: 1.63049\n",
      "Gen Loss: 1.38836 Disc Loss: 0.882539\n",
      "Saved Model\n",
      "Gen Loss: 1.57957 Disc Loss: 0.819915\n",
      "Gen Loss: 0.747214 Disc Loss: 0.883265\n",
      "Gen Loss: 1.50944 Disc Loss: 1.14222\n",
      "Gen Loss: 1.31588 Disc Loss: 0.96595\n",
      "Gen Loss: 1.15933 Disc Loss: 1.03421\n",
      "Gen Loss: 1.66075 Disc Loss: 1.07727\n",
      "Gen Loss: 1.27565 Disc Loss: 0.767797\n",
      "Gen Loss: 0.702846 Disc Loss: 0.977769\n",
      "Gen Loss: 1.27737 Disc Loss: 0.891998\n",
      "Gen Loss: 1.00073 Disc Loss: 0.794389\n",
      "Gen Loss: 1.00222 Disc Loss: 1.1312\n",
      "Gen Loss: 1.59066 Disc Loss: 1.06471\n",
      "Gen Loss: 1.3051 Disc Loss: 0.968029\n",
      "Gen Loss: 1.41365 Disc Loss: 1.02212\n",
      "Gen Loss: 0.808007 Disc Loss: 0.948109\n",
      "Gen Loss: 1.42452 Disc Loss: 0.841952\n",
      "Gen Loss: 0.751768 Disc Loss: 1.03451\n",
      "Gen Loss: 1.37496 Disc Loss: 1.04189\n",
      "Gen Loss: 1.12513 Disc Loss: 1.06999\n",
      "Gen Loss: 1.34549 Disc Loss: 1.02738\n",
      "Gen Loss: 0.730095 Disc Loss: 0.937455\n",
      "Gen Loss: 1.45896 Disc Loss: 0.965204\n",
      "Gen Loss: 0.719168 Disc Loss: 1.07976\n",
      "Gen Loss: 0.423656 Disc Loss: 1.26961\n",
      "Gen Loss: 0.93124 Disc Loss: 1.05201\n",
      "Gen Loss: 0.785626 Disc Loss: 0.867179\n",
      "Gen Loss: 0.980931 Disc Loss: 0.935924\n",
      "Gen Loss: 0.793604 Disc Loss: 0.920493\n",
      "Gen Loss: 1.142 Disc Loss: 1.47207\n",
      "Gen Loss: 1.7646 Disc Loss: 1.07726\n",
      "Gen Loss: 0.990858 Disc Loss: 0.792348\n",
      "Gen Loss: 2.21155 Disc Loss: 1.12653\n",
      "Gen Loss: 1.95434 Disc Loss: 0.910243\n",
      "Gen Loss: 0.817104 Disc Loss: 0.971913\n",
      "Gen Loss: 0.63253 Disc Loss: 0.922445\n",
      "Gen Loss: 1.30064 Disc Loss: 1.04993\n",
      "Gen Loss: 0.697198 Disc Loss: 0.999254\n",
      "Gen Loss: 1.08922 Disc Loss: 0.96426\n",
      "Gen Loss: 1.23467 Disc Loss: 0.884394\n",
      "Gen Loss: 1.36242 Disc Loss: 0.887083\n",
      "Gen Loss: 0.863593 Disc Loss: 0.807705\n",
      "Gen Loss: 0.537233 Disc Loss: 1.00929\n",
      "Gen Loss: 0.536475 Disc Loss: 1.20455\n",
      "Gen Loss: 0.942708 Disc Loss: 0.873835\n",
      "Gen Loss: 1.10553 Disc Loss: 0.947327\n",
      "Gen Loss: 1.63241 Disc Loss: 1.03043\n",
      "Gen Loss: 0.835059 Disc Loss: 1.01865\n",
      "Gen Loss: 0.416899 Disc Loss: 1.54121\n",
      "Gen Loss: 1.04355 Disc Loss: 0.945877\n",
      "Gen Loss: 0.57454 Disc Loss: 1.78801\n",
      "Gen Loss: 0.860088 Disc Loss: 0.937565\n",
      "Gen Loss: 0.854412 Disc Loss: 0.971336\n",
      "Gen Loss: 0.834872 Disc Loss: 0.815051\n",
      "Gen Loss: 0.960869 Disc Loss: 0.836147\n",
      "Gen Loss: 1.30221 Disc Loss: 0.996486\n",
      "Gen Loss: 1.17057 Disc Loss: 1.00958\n",
      "Gen Loss: 1.59142 Disc Loss: 0.952273\n",
      "Gen Loss: 0.349877 Disc Loss: 0.840987\n",
      "Gen Loss: 0.974157 Disc Loss: 1.05366\n",
      "Gen Loss: 1.00285 Disc Loss: 0.760522\n",
      "Gen Loss: 0.687329 Disc Loss: 1.03941\n",
      "Gen Loss: 1.03675 Disc Loss: 0.970725\n",
      "Gen Loss: 0.870268 Disc Loss: 0.850134\n",
      "Gen Loss: 1.61659 Disc Loss: 0.891533\n",
      "Gen Loss: 1.28498 Disc Loss: 0.9277\n",
      "Gen Loss: 1.13327 Disc Loss: 0.822083\n",
      "Gen Loss: 0.915243 Disc Loss: 0.927169\n",
      "Gen Loss: 1.26592 Disc Loss: 0.920385\n",
      "Gen Loss: 1.19814 Disc Loss: 1.03715\n",
      "Gen Loss: 0.842428 Disc Loss: 0.848441\n",
      "Gen Loss: 0.388868 Disc Loss: 1.02199\n",
      "Gen Loss: 0.83832 Disc Loss: 1.18296\n",
      "Gen Loss: 1.03598 Disc Loss: 0.857981\n",
      "Gen Loss: 0.897021 Disc Loss: 0.910559\n",
      "Gen Loss: 1.59307 Disc Loss: 1.20588\n",
      "Gen Loss: 1.03707 Disc Loss: 0.829229\n",
      "Gen Loss: 1.04974 Disc Loss: 1.042\n",
      "Gen Loss: 0.712972 Disc Loss: 1.09014\n",
      "Gen Loss: 0.545047 Disc Loss: 0.87742\n",
      "Gen Loss: 1.14167 Disc Loss: 0.899968\n",
      "Gen Loss: 0.799474 Disc Loss: 0.971746\n",
      "Gen Loss: 0.616116 Disc Loss: 0.829424\n",
      "Gen Loss: 1.18894 Disc Loss: 1.21456\n",
      "Gen Loss: 1.113 Disc Loss: 0.846711\n",
      "Gen Loss: 0.916318 Disc Loss: 0.843975\n",
      "Gen Loss: 0.881222 Disc Loss: 0.871691\n",
      "Gen Loss: 0.880239 Disc Loss: 0.859475\n",
      "Gen Loss: 1.59154 Disc Loss: 0.893666\n",
      "Gen Loss: 0.801321 Disc Loss: 0.905503\n",
      "Gen Loss: 1.06575 Disc Loss: 0.911741\n",
      "Gen Loss: 0.542408 Disc Loss: 1.19423\n",
      "Gen Loss: 0.6113 Disc Loss: 1.13649\n",
      "Gen Loss: 0.559792 Disc Loss: 0.962074\n",
      "Gen Loss: 0.511558 Disc Loss: 1.03421\n",
      "Gen Loss: 0.942515 Disc Loss: 0.92275\n",
      "Gen Loss: 1.89323 Disc Loss: 0.958665\n",
      "Gen Loss: 1.02471 Disc Loss: 0.993513\n",
      "Gen Loss: 1.08281 Disc Loss: 0.848694\n",
      "Gen Loss: 0.608788 Disc Loss: 1.10836\n",
      "Gen Loss: 0.502553 Disc Loss: 1.02676\n",
      "Saved Model\n",
      "Gen Loss: 0.712724 Disc Loss: 0.985378\n",
      "Gen Loss: 1.21843 Disc Loss: 0.820778\n",
      "Gen Loss: 0.979223 Disc Loss: 0.838003\n",
      "Gen Loss: 2.11926 Disc Loss: 1.10595\n",
      "Gen Loss: 0.680214 Disc Loss: 0.881088\n",
      "Gen Loss: 0.80562 Disc Loss: 0.917447\n",
      "Gen Loss: 0.417811 Disc Loss: 1.01882\n",
      "Gen Loss: 1.69036 Disc Loss: 1.04721\n",
      "Gen Loss: 0.839866 Disc Loss: 0.971042\n",
      "Gen Loss: 0.672069 Disc Loss: 1.07565\n",
      "Gen Loss: 0.855037 Disc Loss: 0.845622\n",
      "Gen Loss: 0.931042 Disc Loss: 0.764092\n",
      "Gen Loss: 1.3295 Disc Loss: 1.12826\n",
      "Gen Loss: 0.939751 Disc Loss: 0.848569\n",
      "Gen Loss: 1.11371 Disc Loss: 0.797633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.474967 Disc Loss: 0.955886\n",
      "Gen Loss: 0.787772 Disc Loss: 0.863765\n",
      "Gen Loss: 0.762407 Disc Loss: 0.916176\n",
      "Gen Loss: 0.741618 Disc Loss: 0.852357\n",
      "Gen Loss: 1.07189 Disc Loss: 0.850599\n",
      "Gen Loss: 0.983379 Disc Loss: 0.657732\n",
      "Gen Loss: 0.640358 Disc Loss: 1.03323\n",
      "Gen Loss: 1.20647 Disc Loss: 0.821179\n",
      "Gen Loss: 0.446472 Disc Loss: 1.02764\n",
      "Gen Loss: 1.55053 Disc Loss: 1.19439\n",
      "Gen Loss: 1.04481 Disc Loss: 1.0303\n",
      "Gen Loss: 0.699175 Disc Loss: 0.964975\n",
      "Gen Loss: 1.44253 Disc Loss: 0.991574\n",
      "Gen Loss: 1.04783 Disc Loss: 0.794808\n",
      "Gen Loss: 1.21131 Disc Loss: 1.77478\n",
      "Gen Loss: 1.29237 Disc Loss: 0.810842\n",
      "Gen Loss: 0.8807 Disc Loss: 0.88063\n",
      "Gen Loss: 1.03522 Disc Loss: 0.931449\n",
      "Gen Loss: 0.784129 Disc Loss: 0.821637\n",
      "Gen Loss: 0.988869 Disc Loss: 0.691047\n",
      "Gen Loss: 0.964898 Disc Loss: 0.90026\n",
      "Gen Loss: 1.41618 Disc Loss: 0.803873\n",
      "Gen Loss: 1.12833 Disc Loss: 0.949737\n",
      "Gen Loss: 1.13252 Disc Loss: 1.37351\n",
      "Gen Loss: 1.10929 Disc Loss: 0.977736\n",
      "Gen Loss: 0.76903 Disc Loss: 0.913618\n",
      "Gen Loss: 1.21125 Disc Loss: 1.02559\n",
      "Gen Loss: 0.966247 Disc Loss: 0.833786\n",
      "Gen Loss: 1.6597 Disc Loss: 0.896667\n",
      "Gen Loss: 1.76277 Disc Loss: 0.909862\n",
      "Gen Loss: 1.4554 Disc Loss: 0.904554\n",
      "Gen Loss: 1.12937 Disc Loss: 1.129\n",
      "Gen Loss: 1.49169 Disc Loss: 1.0078\n",
      "Gen Loss: 1.42505 Disc Loss: 0.967471\n",
      "Gen Loss: 1.31641 Disc Loss: 0.908498\n",
      "Gen Loss: 0.764727 Disc Loss: 0.920098\n",
      "Gen Loss: 0.614878 Disc Loss: 0.865333\n",
      "Gen Loss: 0.976987 Disc Loss: 0.90592\n",
      "Gen Loss: 1.50541 Disc Loss: 0.974651\n",
      "Gen Loss: 0.932743 Disc Loss: 0.894051\n",
      "Gen Loss: 1.06667 Disc Loss: 0.905208\n",
      "Gen Loss: 0.59703 Disc Loss: 1.01087\n",
      "Gen Loss: 1.08253 Disc Loss: 1.48179\n",
      "Gen Loss: 0.723337 Disc Loss: 1.20121\n",
      "Gen Loss: 0.803533 Disc Loss: 0.890874\n",
      "Gen Loss: 0.572333 Disc Loss: 1.00069\n",
      "Gen Loss: 0.989488 Disc Loss: 0.74035\n",
      "Gen Loss: 0.467578 Disc Loss: 1.18626\n",
      "Gen Loss: 0.362422 Disc Loss: 1.22495\n",
      "Gen Loss: 0.895035 Disc Loss: 0.797874\n",
      "Gen Loss: 1.46566 Disc Loss: 0.927375\n",
      "Gen Loss: 1.31976 Disc Loss: 0.98472\n",
      "Gen Loss: 1.14823 Disc Loss: 0.897977\n",
      "Gen Loss: 1.56738 Disc Loss: 0.796157\n",
      "Gen Loss: 0.693866 Disc Loss: 0.910425\n",
      "Gen Loss: 1.25936 Disc Loss: 0.83412\n",
      "Gen Loss: 0.674401 Disc Loss: 0.804559\n",
      "Gen Loss: 0.874761 Disc Loss: 0.8936\n",
      "Gen Loss: 1.36169 Disc Loss: 0.820266\n",
      "Gen Loss: 1.1738 Disc Loss: 0.763222\n",
      "Gen Loss: 2.14849 Disc Loss: 1.50542\n",
      "Gen Loss: 0.754466 Disc Loss: 1.05992\n",
      "Gen Loss: 0.933131 Disc Loss: 0.852813\n",
      "Gen Loss: 0.77588 Disc Loss: 0.83838\n",
      "Gen Loss: 0.99708 Disc Loss: 0.79591\n",
      "Gen Loss: 1.0872 Disc Loss: 0.968164\n",
      "Gen Loss: 1.32889 Disc Loss: 1.04465\n",
      "Gen Loss: 0.900766 Disc Loss: 0.929575\n",
      "Gen Loss: 0.937566 Disc Loss: 0.819827\n",
      "Gen Loss: 0.649934 Disc Loss: 1.08768\n",
      "Gen Loss: 1.06343 Disc Loss: 0.839089\n",
      "Gen Loss: 0.922368 Disc Loss: 0.860781\n",
      "Gen Loss: 1.25968 Disc Loss: 0.867893\n",
      "Gen Loss: 0.573524 Disc Loss: 0.820225\n",
      "Gen Loss: 1.23265 Disc Loss: 0.746618\n",
      "Gen Loss: 2.01313 Disc Loss: 1.28727\n",
      "Gen Loss: 0.865793 Disc Loss: 0.91176\n",
      "Gen Loss: 1.84088 Disc Loss: 0.984561\n",
      "Gen Loss: 0.708474 Disc Loss: 1.08392\n",
      "Gen Loss: 0.752263 Disc Loss: 0.967567\n",
      "Gen Loss: 0.81748 Disc Loss: 0.828987\n",
      "Gen Loss: 1.00316 Disc Loss: 0.718824\n",
      "Gen Loss: 1.1867 Disc Loss: 0.779502\n",
      "Gen Loss: 1.4903 Disc Loss: 1.05806\n",
      "Gen Loss: 0.901515 Disc Loss: 0.891357\n",
      "Saved Model\n",
      "Gen Loss: 0.727821 Disc Loss: 1.03128\n",
      "Gen Loss: 1.04928 Disc Loss: 0.895297\n",
      "Gen Loss: 1.75791 Disc Loss: 1.13699\n",
      "Gen Loss: 0.895606 Disc Loss: 0.711322\n",
      "Gen Loss: 1.50002 Disc Loss: 0.994647\n",
      "Gen Loss: 0.972457 Disc Loss: 0.81597\n",
      "Gen Loss: 0.785579 Disc Loss: 0.957156\n",
      "Gen Loss: 0.868752 Disc Loss: 0.830895\n",
      "Gen Loss: 0.84335 Disc Loss: 1.00755\n",
      "Gen Loss: 1.11294 Disc Loss: 0.790112\n",
      "Gen Loss: 1.33962 Disc Loss: 0.803118\n",
      "Gen Loss: 1.45405 Disc Loss: 1.04118\n",
      "Gen Loss: 0.813394 Disc Loss: 0.956815\n",
      "Gen Loss: 1.64714 Disc Loss: 1.03539\n",
      "Gen Loss: 1.07636 Disc Loss: 0.845362\n",
      "Gen Loss: 0.999916 Disc Loss: 0.990049\n",
      "Gen Loss: 0.901797 Disc Loss: 0.976209\n",
      "Gen Loss: 1.00007 Disc Loss: 0.706875\n",
      "Gen Loss: 1.16594 Disc Loss: 0.867908\n",
      "Gen Loss: 0.604774 Disc Loss: 0.964312\n",
      "Gen Loss: 0.704104 Disc Loss: 0.995244\n",
      "Gen Loss: 0.885747 Disc Loss: 0.729673\n",
      "Gen Loss: 0.73454 Disc Loss: 0.805114\n",
      "Gen Loss: 0.637704 Disc Loss: 1.06148\n",
      "Gen Loss: 1.16033 Disc Loss: 0.873092\n",
      "Gen Loss: 1.04463 Disc Loss: 0.917515\n",
      "Gen Loss: 0.625407 Disc Loss: 0.896673\n",
      "Gen Loss: 1.02105 Disc Loss: 0.799125\n",
      "Gen Loss: 1.3048 Disc Loss: 0.80774\n",
      "Gen Loss: 0.907017 Disc Loss: 0.7837\n",
      "Gen Loss: 1.60075 Disc Loss: 0.846748\n",
      "Gen Loss: 1.21556 Disc Loss: 0.998875\n",
      "Gen Loss: 1.5277 Disc Loss: 0.791066\n",
      "Gen Loss: 0.439613 Disc Loss: 1.10402\n",
      "Gen Loss: 0.782037 Disc Loss: 0.83675\n",
      "Gen Loss: 1.19923 Disc Loss: 0.872408\n",
      "Gen Loss: 1.52905 Disc Loss: 0.856109\n",
      "Gen Loss: 1.51471 Disc Loss: 0.850357\n",
      "Gen Loss: 0.808949 Disc Loss: 0.789916\n",
      "Gen Loss: 1.10896 Disc Loss: 0.998062\n",
      "Gen Loss: 1.07484 Disc Loss: 0.800754\n",
      "Gen Loss: 0.653531 Disc Loss: 0.820634\n",
      "Gen Loss: 0.980335 Disc Loss: 0.912306\n",
      "Gen Loss: 0.562698 Disc Loss: 1.18578\n",
      "Gen Loss: 0.842482 Disc Loss: 0.89511\n",
      "Gen Loss: 0.611933 Disc Loss: 0.848291\n",
      "Gen Loss: 0.898333 Disc Loss: 0.796635\n",
      "Gen Loss: 1.08239 Disc Loss: 0.948434\n",
      "Gen Loss: 0.807089 Disc Loss: 0.997818\n",
      "Gen Loss: 0.574364 Disc Loss: 0.963697\n",
      "Gen Loss: 0.746972 Disc Loss: 1.05688\n",
      "Gen Loss: 1.75222 Disc Loss: 0.95676\n",
      "Gen Loss: 0.795194 Disc Loss: 0.800248\n",
      "Gen Loss: 0.902042 Disc Loss: 0.902754\n",
      "Gen Loss: 1.07658 Disc Loss: 0.977381\n",
      "Gen Loss: 0.838442 Disc Loss: 0.833791\n",
      "Gen Loss: 1.22142 Disc Loss: 0.987412\n",
      "Gen Loss: 0.781837 Disc Loss: 0.946419\n",
      "Gen Loss: 0.677635 Disc Loss: 0.935011\n",
      "Gen Loss: 0.841102 Disc Loss: 0.842246\n",
      "Gen Loss: 0.970898 Disc Loss: 0.840075\n",
      "Gen Loss: 0.496997 Disc Loss: 0.931116\n",
      "Gen Loss: 1.16509 Disc Loss: 0.82426\n",
      "Gen Loss: 0.780951 Disc Loss: 0.919798\n",
      "Gen Loss: 0.626223 Disc Loss: 0.863882\n",
      "Gen Loss: 0.853166 Disc Loss: 0.743862\n",
      "Gen Loss: 1.46903 Disc Loss: 1.23694\n",
      "Gen Loss: 0.750842 Disc Loss: 1.06511\n",
      "Gen Loss: 0.776285 Disc Loss: 0.927398\n",
      "Gen Loss: 1.13229 Disc Loss: 1.08425\n",
      "Gen Loss: 0.624591 Disc Loss: 0.913813\n",
      "Gen Loss: 0.465957 Disc Loss: 1.31107\n",
      "Gen Loss: 0.369777 Disc Loss: 1.02603\n",
      "Gen Loss: 0.975342 Disc Loss: 0.747784\n",
      "Gen Loss: 1.32828 Disc Loss: 1.25037\n",
      "Gen Loss: 1.3223 Disc Loss: 0.9231\n",
      "Gen Loss: 1.24015 Disc Loss: 0.922488\n",
      "Gen Loss: 1.24093 Disc Loss: 1.1921\n",
      "Gen Loss: 0.828356 Disc Loss: 0.770161\n",
      "Gen Loss: 0.565215 Disc Loss: 0.97381\n",
      "Gen Loss: 0.755021 Disc Loss: 0.830455\n",
      "Gen Loss: 0.935219 Disc Loss: 0.891741\n",
      "Gen Loss: 1.58088 Disc Loss: 0.767181\n",
      "Gen Loss: 1.29747 Disc Loss: 1.13079\n",
      "Gen Loss: 0.872008 Disc Loss: 0.833959\n",
      "Gen Loss: 0.551716 Disc Loss: 1.08207\n",
      "Gen Loss: 0.931938 Disc Loss: 0.945908\n",
      "Gen Loss: 1.09088 Disc Loss: 0.743457\n",
      "Gen Loss: 1.44157 Disc Loss: 0.96186\n",
      "Gen Loss: 0.490324 Disc Loss: 0.953478\n",
      "Gen Loss: 1.15858 Disc Loss: 0.790324\n",
      "Gen Loss: 0.959513 Disc Loss: 0.833531\n",
      "Gen Loss: 1.17727 Disc Loss: 0.719906\n",
      "Gen Loss: 0.782715 Disc Loss: 0.831716\n",
      "Gen Loss: 1.12775 Disc Loss: 0.836242\n",
      "Gen Loss: 1.11103 Disc Loss: 0.899006\n",
      "Gen Loss: 0.665274 Disc Loss: 0.908833\n",
      "Gen Loss: 0.828059 Disc Loss: 0.757332\n",
      "Gen Loss: 0.474219 Disc Loss: 1.04207\n",
      "Gen Loss: 0.790288 Disc Loss: 0.815884\n",
      "Saved Model\n",
      "Gen Loss: 1.02802 Disc Loss: 1.18964\n",
      "Gen Loss: 1.11415 Disc Loss: 0.834519\n",
      "Gen Loss: 1.05988 Disc Loss: 0.881232\n",
      "Gen Loss: 0.81573 Disc Loss: 0.827652\n",
      "Gen Loss: 0.809054 Disc Loss: 0.802565\n",
      "Gen Loss: 0.559573 Disc Loss: 1.05744\n",
      "Gen Loss: 0.540408 Disc Loss: 0.99936\n",
      "Gen Loss: 0.827925 Disc Loss: 1.02371\n",
      "Gen Loss: 0.739226 Disc Loss: 0.869539\n",
      "Gen Loss: 1.44234 Disc Loss: 1.00666\n",
      "Gen Loss: 1.36942 Disc Loss: 0.8693\n",
      "Gen Loss: 0.792391 Disc Loss: 0.863029\n",
      "Gen Loss: 0.891028 Disc Loss: 1.05383\n",
      "Gen Loss: 0.974159 Disc Loss: 0.948348\n",
      "Gen Loss: 0.938257 Disc Loss: 0.877299\n",
      "Gen Loss: 0.854422 Disc Loss: 0.994886\n",
      "Gen Loss: 1.86418 Disc Loss: 0.975828\n",
      "Gen Loss: 0.75161 Disc Loss: 1.12867\n",
      "Gen Loss: 0.570984 Disc Loss: 0.870592\n",
      "Gen Loss: 0.76793 Disc Loss: 0.848089\n",
      "Gen Loss: 0.873224 Disc Loss: 0.884271\n",
      "Gen Loss: 1.2081 Disc Loss: 0.843015\n",
      "Gen Loss: 1.00864 Disc Loss: 0.945197\n",
      "Gen Loss: 0.749899 Disc Loss: 0.810985\n",
      "Gen Loss: 1.48816 Disc Loss: 0.678341\n",
      "Gen Loss: 1.1397 Disc Loss: 1.00692\n",
      "Gen Loss: 2.25276 Disc Loss: 1.09926\n",
      "Gen Loss: 0.406981 Disc Loss: 1.23032\n",
      "Gen Loss: 1.14334 Disc Loss: 0.724655\n",
      "Gen Loss: 1.09427 Disc Loss: 0.785472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen Loss: 0.619443 Disc Loss: 0.805264\n",
      "Gen Loss: 0.995207 Disc Loss: 1.05934\n",
      "Gen Loss: 0.704235 Disc Loss: 0.976222\n",
      "Gen Loss: 0.528885 Disc Loss: 1.09738\n",
      "Gen Loss: 0.73238 Disc Loss: 0.851952\n",
      "Gen Loss: 1.04284 Disc Loss: 0.878985\n",
      "Gen Loss: 0.413167 Disc Loss: 1.04069\n",
      "Gen Loss: 0.517384 Disc Loss: 1.03997\n",
      "Gen Loss: 1.08662 Disc Loss: 0.866366\n",
      "Gen Loss: 0.900583 Disc Loss: 1.00995\n",
      "Gen Loss: 0.656989 Disc Loss: 0.856748\n",
      "Gen Loss: 1.02754 Disc Loss: 0.821781\n",
      "Gen Loss: 0.755045 Disc Loss: 0.934867\n",
      "Gen Loss: 0.86583 Disc Loss: 1.01934\n",
      "Gen Loss: 0.840763 Disc Loss: 0.76218\n",
      "Gen Loss: 0.451582 Disc Loss: 1.45644\n",
      "Gen Loss: 1.007 Disc Loss: 1.20165\n",
      "Gen Loss: 0.873184 Disc Loss: 1.0102\n",
      "Gen Loss: 0.892128 Disc Loss: 0.862875\n",
      "Gen Loss: 0.916778 Disc Loss: 0.91648\n",
      "Gen Loss: 1.20666 Disc Loss: 1.02705\n",
      "Gen Loss: 0.736739 Disc Loss: 0.945186\n",
      "Gen Loss: 0.669741 Disc Loss: 0.886686\n",
      "Gen Loss: 0.731428 Disc Loss: 0.867677\n",
      "Gen Loss: 0.853958 Disc Loss: 0.917308\n",
      "Gen Loss: 0.645725 Disc Loss: 0.926677\n",
      "Gen Loss: 0.495227 Disc Loss: 1.10038\n",
      "Gen Loss: 0.601226 Disc Loss: 0.987852\n",
      "Gen Loss: 1.04412 Disc Loss: 0.855266\n",
      "Gen Loss: 1.25914 Disc Loss: 1.05253\n",
      "Gen Loss: 0.607804 Disc Loss: 0.863012\n",
      "Gen Loss: 1.4898 Disc Loss: 0.870287\n",
      "Gen Loss: 1.03875 Disc Loss: 1.20765\n",
      "Gen Loss: 0.538533 Disc Loss: 1.02101\n",
      "Gen Loss: 1.17357 Disc Loss: 0.718826\n",
      "Gen Loss: 1.03919 Disc Loss: 0.769328\n",
      "Gen Loss: 0.871936 Disc Loss: 0.9088\n",
      "Gen Loss: 0.951731 Disc Loss: 0.721416\n",
      "Gen Loss: 0.504894 Disc Loss: 0.989297\n",
      "Gen Loss: 0.545243 Disc Loss: 1.10318\n",
      "Gen Loss: 1.28526 Disc Loss: 0.835194\n",
      "Gen Loss: 0.984609 Disc Loss: 0.881454\n",
      "Gen Loss: 0.57254 Disc Loss: 0.999645\n",
      "Gen Loss: 1.01269 Disc Loss: 0.968051\n",
      "Gen Loss: 0.674306 Disc Loss: 1.11135\n",
      "Gen Loss: 0.932744 Disc Loss: 0.806362\n",
      "Gen Loss: 0.871441 Disc Loss: 0.858793\n",
      "Gen Loss: 0.616181 Disc Loss: 1.13391\n",
      "Gen Loss: 0.900632 Disc Loss: 0.845131\n",
      "Gen Loss: 0.833826 Disc Loss: 0.744519\n",
      "Gen Loss: 0.628919 Disc Loss: 0.868539\n",
      "Gen Loss: 0.785875 Disc Loss: 0.863184\n",
      "Gen Loss: 0.830725 Disc Loss: 0.928726\n",
      "Gen Loss: 0.876693 Disc Loss: 0.866644\n",
      "Gen Loss: 0.770983 Disc Loss: 0.811061\n",
      "Gen Loss: 0.634805 Disc Loss: 0.889971\n",
      "Gen Loss: 0.696103 Disc Loss: 0.864476\n",
      "Gen Loss: 0.69453 Disc Loss: 0.800334\n",
      "Gen Loss: 0.475038 Disc Loss: 0.998836\n",
      "Gen Loss: 0.954292 Disc Loss: 0.79193\n",
      "Gen Loss: 0.813147 Disc Loss: 1.06932\n",
      "Gen Loss: 0.407729 Disc Loss: 1.34693\n",
      "Gen Loss: 0.76362 Disc Loss: 0.836428\n",
      "Gen Loss: 1.38202 Disc Loss: 0.83633\n",
      "Gen Loss: 1.21877 Disc Loss: 1.07975\n",
      "Gen Loss: 1.05681 Disc Loss: 0.894061\n",
      "Gen Loss: 1.15908 Disc Loss: 0.855641\n",
      "Gen Loss: 0.993168 Disc Loss: 0.834955\n",
      "Gen Loss: 0.803751 Disc Loss: 1.13614\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128 #Size of image batch to apply at each iteration.\n",
    "iterations = 5000 #Total number of iterations to use.\n",
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    for i in range(iterations):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        xs,_ = mnist.train.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        \n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        _,dLoss = sess.run([update_D,d_loss],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs}) #Update the generator, twice for good measure.\n",
    "        _,gLoss = sess.run([update_G,g_loss],feed_dict={z_in:zs})\n",
    "        if i % 10 == 0:\n",
    "            print(\"Gen Loss: \" + str(gLoss) + \" Disc Loss: \" + str(dLoss))\n",
    "            z2 = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate another z batch\n",
    "            newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "            if not os.path.exists(sample_directory):\n",
    "                os.makedirs(sample_directory)\n",
    "            #Save sample generator images for viewing training progress.\n",
    "            save_images(np.reshape(newZ[0:36],[36,32,32]),[6,6],sample_directory+'/fig'+str(i)+'.png')\n",
    "        if i % 1000 == 0 and i != 0:\n",
    "            if not os.path.exists(model_directory):\n",
    "                os.makedirs(model_directory)\n",
    "            saver.save(sess,model_directory+'/model-'+str(i)+'.cptk')\n",
    "            print(\"Saved Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a trained network\n",
    "Once we have a trained model saved, we may want to use it to generate new images, and explore the representation it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/model-4000.cptk\n"
     ]
    }
   ],
   "source": [
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to load trained model from.\n",
    "batch_size_sample = 36\n",
    "\n",
    "path = model_directory  + '/model-4000.cptk'\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    #Reload the model.\n",
    "    print( 'Loading Model...')\n",
    "    saver.restore(sess, save_path=path)\n",
    "    \n",
    "    zs = np.random.uniform(-1.0,1.0,size=[batch_size_sample,z_size]).astype(np.float32) #Generate a random z batch\n",
    "    newZ = sess.run(Gz,feed_dict={z_in:z2}) #Use new z to get sample images from generator.\n",
    "    if not os.path.exists(sample_directory):\n",
    "        os.makedirs(sample_directory)\n",
    "    save_images(np.reshape(newZ[0:batch_size_sample],[36,32,32]),[6,6],sample_directory+'/fig.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 (2 points)\n",
    "Run a couple of iterations and visualize examples generated by the generator (Could be found in ./fig folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMAAAADACAAAAAB3tzPbAAA8lElEQVR4nO29d5hcxZU3fKrq5ns7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\nrkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='figs/fig.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 (5 points)\n",
    "Evaluate discrimator accuracy in the pre-trained model on any representative subsample of fashion-minst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model...\n",
      "INFO:tensorflow:Restoring parameters from ./models/model-4000.cptk\n",
      "accuracy =  0.90228125\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128 #Size of image batch to apply at each iteration.\n",
    "iterations = 1000 #Total number of iterations to use.\n",
    "sample_directory = './figs' #Directory to save sample images from generator in.\n",
    "model_directory = './models' #Directory to save trained model to.\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:  \n",
    "    sess.run(init)\n",
    "    #Reload the model.\n",
    "    print( 'Loading Model...')\n",
    "    saver.restore(sess, save_path=path)\n",
    "    \n",
    "    cnt_rght = 0\n",
    "    cnt_wrng = 0\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        zs = np.random.uniform(-1.0,1.0,size=[batch_size,z_size]).astype(np.float32) #Generate a random z batch\n",
    "        xs,_ = mnist.test.next_batch(batch_size) #Draw a sample batch from MNIST dataset.\n",
    "        xs = (np.reshape(xs,[batch_size,28,28,1]) - 0.5) * 2.0 #Transform it to be between -1 and 1\n",
    "        xs = np.lib.pad(xs, ((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=(-1, -1)) #Pad the images so the are 32x32\n",
    "        \n",
    "        \n",
    "        dx, dg = sess.run([Dx, Dg],feed_dict={z_in:zs,real_in:xs}) #Update the discriminator\n",
    "        \n",
    "        for dxj in dx:\n",
    "            if dxj > 0.5:\n",
    "                cnt_rght += 1\n",
    "            else:\n",
    "                cnt_wrng += 1\n",
    "        \n",
    "        for dgj in dg:\n",
    "            if dgj < 0.5:\n",
    "                cnt_rght += 1\n",
    "            else:\n",
    "                cnt_wrng += 1\n",
    "        \n",
    "    print(\"accuracy = \", cnt_rght/(cnt_rght + cnt_wrng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
