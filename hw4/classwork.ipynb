{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to prevent tensorflow from allocating the totality of a GPU memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assume that you have 12GB of GPU memory and want to allocate ~4GB:\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TensorFlow-Slim\n",
    "\n",
    "TF-Slim is a lightweight library for defining, training and evaluating complex\n",
    "models in TensorFlow. Components of tf-slim can be freely mixed with native\n",
    "tensorflow, as well as other frameworks, such as tf.contrib.learn.\n",
    "\n",
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why TF-Slim?\n",
    "\n",
    "TF-Slim is a library that makes building, training and evaluation neural\n",
    "networks simple:\n",
    "\n",
    "* Allows the user to define models much more compactly by eliminating\n",
    "boilerplate code. This is accomplished through the use of\n",
    "[argument scoping](https://www.tensorflow.org/code/tensorflow/contrib/framework/python/ops/arg_scope.py)\n",
    "and numerous high level\n",
    "[layers](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "and\n",
    "[variables](https://www.tensorflow.org/code/tensorflow/contrib/framework/python/ops/variables.py).\n",
    "These tools increase readability and maintainability, reduce the likelihood\n",
    "of an error from copy-and-pasting hyperparameter values and simplifies\n",
    "hyperparameter tuning.\n",
    "* Makes developing models simple by providing commonly used\n",
    "[regularizers](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/regularizers.py).\n",
    "* Several widely used computer vision models (e.g., VGG, AlexNet) have been\n",
    "developed in slim, and are\n",
    "[available](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/nets/)\n",
    "to users. These can either be used as black boxes, or can be extended in various\n",
    "ways, e.g., by adding \"multiple heads\" to different internal layers.\n",
    "* Slim makes it easy to extend complex models, and to warm start training\n",
    "algorithms by using pieces of pre-existing model checkpoints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the various components of TF-Slim?\n",
    "\n",
    "TF-Slim is composed of several parts which were design to exist independently.\n",
    "These include the following main pieces (explained in detail below).\n",
    "\n",
    "* [arg_scope](https://www.tensorflow.org/code/tensorflow/contrib/framework/python/ops/arg_scope.py):\n",
    "provides a new scope named `arg_scope` that allows a user to define default\n",
    "arguments for specific operations within that scope.\n",
    "* [data](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/data/):\n",
    "contains TF-slim's\n",
    "[dataset](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/data/dataset.py)\n",
    "definition,\n",
    "[data providers](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/data/data_provider.py),\n",
    "[parallel_reader](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/data/parallel_reader.py),\n",
    "and\n",
    "[decoding](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/data/data_decoder.py)\n",
    "utilities.\n",
    "* [evaluation](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/evaluation.py):\n",
    "contains routines for evaluating models.\n",
    "* [layers](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py):\n",
    "contains high level layers for building models using tensorflow.\n",
    "* [learning](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/learning.py):\n",
    "contains routines for training models.\n",
    "* [losses](https://www.tensorflow.org/code/tensorflow/contrib/losses/python/losses/loss_ops.py):\n",
    "contains commonly used loss functions.\n",
    "* [metrics](https://www.tensorflow.org/code/tensorflow/contrib/metrics/python/ops/metric_ops.py):\n",
    "contains popular evaluation metrics.\n",
    "* [nets](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/nets/):\n",
    "contains popular network definitions such as\n",
    "[VGG](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/nets/vgg.py)\n",
    "and\n",
    "[AlexNet](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/nets/alexnet.py)\n",
    "models.\n",
    "* [queues](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/queues.py):\n",
    "provides a context manager for easily and safely starting and closing\n",
    "QueueRunners.\n",
    "* [regularizers](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/regularizers.py):\n",
    "contains weight regularizers.\n",
    "* [variables](https://www.tensorflow.org/code/tensorflow/contrib/framework/python/ops/variables.py):\n",
    "provides convenience wrappers for variable creation and manipulation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Models\n",
    "\n",
    "Models can be succinctly defined using TF-Slim by combining its variables,\n",
    "layers and scopes. Each of these elements are defined below.\n",
    "\n",
    "### Variables\n",
    "\n",
    "Creating\n",
    "[`Variables`](https://www.tensorflow.org/how_tos/variables/index.html)\n",
    "in native tensorflow requires either a predefined value or an initialization\n",
    "mechanism (e.g. randomly sampled from a Gaussian). Furthermore, if a variable\n",
    "needs to be created\n",
    "on a specific device, such as a GPU, the specification must be\n",
    "[made explicit](https://www.tensorflow.org/how_tos/using_gpu/index.html).\n",
    "To alleviate the code required for variable creation, TF-Slim provides a set\n",
    "of thin wrapper functions in\n",
    "[variables.py](https://www.tensorflow.org/code/tensorflow/contrib/framework/python/ops/variables.py)\n",
    "which allow callers to easily define variables.\n",
    "\n",
    "For example, to create a `weight` variable, initialize it using a truncated\n",
    "normal distribution, regularize it with an `l2_loss` and place it on the `CPU`,\n",
    "one need only declare the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = slim.variable('weights',\n",
    "                             shape=[10, 10, 3 , 3],\n",
    "                             initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                             regularizer=slim.l2_regularizer(0.05),\n",
    "                             device='/CPU:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in native TensorFlow, there are two types of variables: regular\n",
    "variables and local (transient) variables. The vast majority of variables are\n",
    "regular variables: once created, they can be saved to disk using a\n",
    "[saver](https://www.tensorflow.org/api_docs/python/tf/train/Saver).\n",
    "Local variables are those variables that only exist for the duration of a\n",
    "session and are not saved to disk.\n",
    "\n",
    "TF-Slim further differentiates variables by defining *model variables*, which\n",
    "are variables that represent parameters of a model. Model variables are\n",
    "trained or fine-tuned during learning and are loaded\n",
    "from a checkpoint during evaluation or inference. Examples include the variables\n",
    "created by a `slim.fully_connected` or `slim.conv2d` layer. Non-model variables\n",
    "are all other variables that are used during learning or evaluation but are not\n",
    "required for actually performing inference. For example, the `global_step` is\n",
    "a variable using during learning and evaluation but it is not actually part of\n",
    "the model. Similarly, moving average variables might mirror model variables,\n",
    "but the moving averages are not themselves model variables.\n",
    "\n",
    "Both model variables and regular variables can be easily created and retrieved\n",
    "via TF-Slim:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Model Variables\n",
    "weights = slim.model_variable('weights',\n",
    "                              shape=[10, 10, 3 , 3],\n",
    "                              initializer=tf.truncated_normal_initializer(stddev=0.1),\n",
    "                              regularizer=slim.l2_regularizer(0.05),\n",
    "                              device='/CPU:0')\n",
    "model_variables = slim.get_model_variables()\n",
    "\n",
    "# Regular variables\n",
    "my_var = slim.variable('my_var',\n",
    "                       shape=[20, 1],\n",
    "                       initializer=tf.zeros_initializer())\n",
    "regular_variables_and_model_variables = slim.get_variables()\n",
    "```\n",
    "\n",
    "How does this work? When you create a model variable via TF-Slim's layers or\n",
    "directly via the `slim.model_variable` function, TF-Slim adds the variable to\n",
    "a the `tf.GraphKeys.MODEL_VARIABLES` collection. What if you have your own\n",
    "custom layers or variable creation routine but still want TF-Slim to manage or\n",
    "be aware of your model variables? TF-Slim provides a convenience function for\n",
    "adding the model variable to its collection:\n",
    "\n",
    "```python\n",
    "my_model_variable = CreateViaCustomCode()\n",
    "\n",
    "# Letting TF-Slim know about the additional variable.\n",
    "slim.add_model_variable(my_model_variable)\n",
    "```\n",
    "\n",
    "\n",
    "### Layers\n",
    "\n",
    "While the set of TensorFlow operations is quite extensive, developers of\n",
    "neural networks typically think of models in terms of higher level concepts\n",
    "like \"layers\", \"losses\", \"metrics\", and \"networks\". A layer,\n",
    "such as a Convolutional Layer, a Fully Connected Layer or a BatchNorm Layer\n",
    "are more abstract than a single TensorFlow operation and typically involve\n",
    "several operations. Furthermore, a layer usually (but not always) has\n",
    "variables (tunable parameters) associated with it, unlike more primitive\n",
    "operations. For example, a Convolutional Layer in a neural network\n",
    "is composed of several low level operations:\n",
    "\n",
    "1. Creating the weight and bias variables\n",
    "2. Convolving the weights with the input from the previous layer\n",
    "3. Adding the biases to the result of the convolution.\n",
    "4. Applying an activation function.\n",
    "\n",
    "Using only plain TensorFlow code, this can be rather laborious:\n",
    "\n",
    "```python\n",
    "input = ...\n",
    "with tf.name_scope('conv1_1') as scope:\n",
    "  kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n",
    "                                           stddev=1e-1), name='weights')\n",
    "  conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "  biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n",
    "                       trainable=True, name='biases')\n",
    "  bias = tf.nn.bias_add(conv, biases)\n",
    "  conv1 = tf.nn.relu(bias, name=scope)\n",
    "```\n",
    "\n",
    "To alleviate the need to duplicate this code repeatedly, TF-Slim provides a\n",
    "number of convenient operations defined at the more abstract level of\n",
    "neural network layers. For example, compare the code above to an invocation\n",
    "of the corresponding TF-Slim code:\n",
    "\n",
    "```python\n",
    "input = ...\n",
    "net = slim.conv2d(input, 128, [3, 3], scope='conv1_1')\n",
    "```\n",
    "\n",
    "TF-Slim provides standard implementations for numerous components for building\n",
    "neural networks. These include:\n",
    "\n",
    "Layer | TF-Slim\n",
    "------- | --------\n",
    "BiasAdd  | [slim.bias_add](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "BatchNorm  | [slim.batch_norm](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "Conv2d | [slim.conv2d](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "Conv2dInPlane | [slim.conv2d_in_plane](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "Conv2dTranspose (Deconv) | [slim.conv2d_transpose](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "FullyConnected | [slim.fully_connected](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "AvgPool2D | [slim.avg_pool2d](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "Dropout| [slim.dropout](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "Flatten | [slim.flatten](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "MaxPool2D | [slim.max_pool2d](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "OneHotEncoding | [slim.one_hot_encoding](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "SeparableConv2 | [slim.separable_conv2d](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "UnitNorm | [slim.unit_norm](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "\n",
    "TF-Slim also provides two meta-operations called `repeat` and `stack` that\n",
    "allow users to repeatedly perform the same operation. For example, consider the\n",
    "following snippet from the\n",
    "[VGG](https://www.robots.ox.ac.uk/~vgg/research/very_deep/) network whose layers\n",
    "perform several convolutions in a row between pooling layers:\n",
    "\n",
    "```python\n",
    "net = ...\n",
    "net = slim.conv2d(net, 256, [3, 3], scope='conv3_1')\n",
    "net = slim.conv2d(net, 256, [3, 3], scope='conv3_2')\n",
    "net = slim.conv2d(net, 256, [3, 3], scope='conv3_3')\n",
    "net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "```\n",
    "\n",
    "One way to reduce this code duplication would be via a `for` loop:\n",
    "\n",
    "```python\n",
    "net = ...\n",
    "for i in range(3):\n",
    "  net = slim.conv2d(net, 256, [3, 3], scope='conv3_' % (i+1))\n",
    "net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "```\n",
    "\n",
    "This can be made even cleaner by using TF-Slim's `repeat` operation:\n",
    "\n",
    "```python\n",
    "net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "```\n",
    "\n",
    "Notice that the `slim.repeat` not only applies the same argument in-line, it\n",
    "also is smart enough to unroll the scopes such that the scopes assigned to each\n",
    "subsequent call of `slim.conv2d` are appended with an underscore and iteration\n",
    "number. More concretely, the scopes in the example above would be named\n",
    "'conv3/conv3_1', 'conv3/conv3_2' and 'conv3/conv3_3'.\n",
    "\n",
    "Furthermore, TF-Slim's `slim.stack` operator allows a caller to repeatedly apply\n",
    "the same operation with different arguments to create a *stack* or tower of\n",
    "layers. `slim.stack` also creates a new `tf.variable_scope` for each\n",
    "operation created. For example, a simple way to create a Multi-Layer Perceptron\n",
    "(MLP):\n",
    "\n",
    "```python\n",
    "# Verbose way:\n",
    "x = slim.fully_connected(x, 32, scope='fc/fc_1')\n",
    "x = slim.fully_connected(x, 64, scope='fc/fc_2')\n",
    "x = slim.fully_connected(x, 128, scope='fc/fc_3')\n",
    "\n",
    "# Equivalent, TF-Slim way using slim.stack:\n",
    "slim.stack(x, slim.fully_connected, [32, 64, 128], scope='fc')\n",
    "```\n",
    "\n",
    "In this example, `slim.stack` calls `slim.fully_connected` three times passing\n",
    "the output of one invocation of the function to the next. However, the number of\n",
    "hidden units in each invocation changes from 32 to 64 to 128. Similarly, one\n",
    "can use stack to simplify a tower of multiple convolutions:\n",
    "\n",
    "```python\n",
    "# Verbose way:\n",
    "x = slim.conv2d(x, 32, [3, 3], scope='core/core_1')\n",
    "x = slim.conv2d(x, 32, [1, 1], scope='core/core_2')\n",
    "x = slim.conv2d(x, 64, [3, 3], scope='core/core_3')\n",
    "x = slim.conv2d(x, 64, [1, 1], scope='core/core_4')\n",
    "\n",
    "# Using stack:\n",
    "slim.stack(x, slim.conv2d, [(32, [3, 3]), (32, [1, 1]), (64, [3, 3]), (64, [1, 1])], scope='core')\n",
    "```\n",
    "\n",
    "### Scopes\n",
    "\n",
    "In addition to the types of scope mechanisms in TensorFlow\n",
    "([name_scope](https://www.tensorflow.org/api_docs/python/tf/name_scope),\n",
    "[variable_scope](https://www.tensorflow.org/api_docs/python/tf/variable_scope),\n",
    "TF-Slim adds a new scoping mechanism called\n",
    "[arg_scope](https://www.tensorflow.org/api_docs/python/tf/contrib/framework/arg_scope),\n",
    "This new scope allows a user to specify one or more operations and a set of\n",
    "arguments which will be passed to each of the operations defined in the\n",
    "`arg_scope`. This functionality is best illustrated by example. Consider the\n",
    "following code snippet:\n",
    "\n",
    "\n",
    "```python\n",
    "net = slim.conv2d(inputs, 64, [11, 11], 4, padding='SAME',\n",
    "                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                  weights_regularizer=slim.l2_regularizer(0.0005), scope='conv1')\n",
    "net = slim.conv2d(net, 128, [11, 11], padding='VALID',\n",
    "                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                  weights_regularizer=slim.l2_regularizer(0.0005), scope='conv2')\n",
    "net = slim.conv2d(net, 256, [11, 11], padding='SAME',\n",
    "                  weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                  weights_regularizer=slim.l2_regularizer(0.0005), scope='conv3')\n",
    "```\n",
    "\n",
    "It should be clear that these three convolution layers share many of the same\n",
    "hyperparameters. Two have the same padding, all three have the same\n",
    "weights_initializer and weight_regularizer. This code is hard to read and\n",
    "contains a lot of repeated values that should be factored out. One solution\n",
    "would be to specify default values using variables:\n",
    "\n",
    "```python\n",
    "padding = 'SAME'\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "regularizer = slim.l2_regularizer(0.0005)\n",
    "net = slim.conv2d(inputs, 64, [11, 11], 4,\n",
    "                  padding=padding,\n",
    "                  weights_initializer=initializer,\n",
    "                  weights_regularizer=regularizer,\n",
    "                  scope='conv1')\n",
    "net = slim.conv2d(net, 128, [11, 11],\n",
    "                  padding='VALID',\n",
    "                  weights_initializer=initializer,\n",
    "                  weights_regularizer=regularizer,\n",
    "                  scope='conv2')\n",
    "net = slim.conv2d(net, 256, [11, 11],\n",
    "                  padding=padding,\n",
    "                  weights_initializer=initializer,\n",
    "                  weights_regularizer=regularizer,\n",
    "                  scope='conv3')\n",
    "```\n",
    "\n",
    "This solution ensures that all three convolutions share the exact same parameter\n",
    "values but doesn't reduce completely the code clutter. By using an `arg_scope`,\n",
    "we can both ensure that each layer uses the same values and simplify the code:\n",
    "\n",
    "```python\n",
    "  with slim.arg_scope([slim.conv2d], padding='SAME',\n",
    "                      weights_initializer=tf.truncated_normal_initializer(stddev=0.01)\n",
    "                      weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "    net = slim.conv2d(inputs, 64, [11, 11], scope='conv1')\n",
    "    net = slim.conv2d(net, 128, [11, 11], padding='VALID', scope='conv2')\n",
    "    net = slim.conv2d(net, 256, [11, 11], scope='conv3')\n",
    "```\n",
    "\n",
    "As the example illustrates, the use of arg_scope makes the code cleaner,\n",
    "simpler and easier to maintain. Notice that while argument values are specified\n",
    "in the arg_scope, they can be overwritten locally. In particular, while\n",
    "the padding argument has been set to 'SAME', the second convolution overrides\n",
    "it with the value of 'VALID'.\n",
    "\n",
    "One can also nest `arg_scopes` and use multiple operations in the same scope.\n",
    "For example:\n",
    "\n",
    "```python\n",
    "with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                      weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "  with slim.arg_scope([slim.conv2d], stride=1, padding='SAME'):\n",
    "    net = slim.conv2d(inputs, 64, [11, 11], 4, padding='VALID', scope='conv1')\n",
    "    net = slim.conv2d(net, 256, [5, 5],\n",
    "                      weights_initializer=tf.truncated_normal_initializer(stddev=0.03),\n",
    "                      scope='conv2')\n",
    "    net = slim.fully_connected(net, 1000, activation_fn=None, scope='fc')\n",
    "```\n",
    "\n",
    "In this example, the first `arg_scope` applies the same `weights_initializer`\n",
    "and `weights_regularizer` arguments to the `conv2d` and `fully_connected` layers\n",
    "in its scope. In the second `arg_scope`, additional default arguments to\n",
    "`conv2d` only are specified.\n",
    "\n",
    "### Working Example: Specifying the VGG16 Layers\n",
    "\n",
    "By combining TF-Slim Variables, Operations and scopes, we can write a normally\n",
    "very complex network with very few lines of code. For example, the entire\n",
    "[VGG](https://www.robots.ox.ac.uk/~vgg/research/very_deep/) architecture can be\n",
    "defined with just the following snippet:\n",
    "\n",
    "```python\n",
    "def vgg16(inputs):\n",
    "  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                      activation_fn=tf.nn.relu,\n",
    "                      weights_initializer=tf.truncated_normal_initializer(0.0, 0.01),\n",
    "                      weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "    net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1')\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "    net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "    net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool3')\n",
    "    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "    net = slim.repeat(net, 3, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool5')\n",
    "    net = slim.fully_connected(net, 4096, scope='fc6')\n",
    "    net = slim.dropout(net, 0.5, scope='dropout6')\n",
    "    net = slim.fully_connected(net, 4096, scope='fc7')\n",
    "    net = slim.dropout(net, 0.5, scope='dropout7')\n",
    "    net = slim.fully_connected(net, 1000, activation_fn=None, scope='fc8')\n",
    "  return net\n",
    "```\n",
    "\n",
    "## Training Models\n",
    "\n",
    "Training Tensorflow models requires a model, a loss function, the gradient\n",
    "computation and a training routine that iteratively computes the gradients\n",
    "of the model weights relative to the loss and updates the weights accordingly.\n",
    "TF-Slim provides both common loss functions and a set of helper functions\n",
    "that run the training and evaluation routines.\n",
    "\n",
    "### Losses\n",
    "\n",
    "The loss function defines a quantity that we want to minimize. For\n",
    "classification problems, this is typically the cross entropy between the true\n",
    "distribution and the predicted probability distribution across\n",
    "classes. For regression problems, this is often the sum-of-squares differences\n",
    "between the predicted and true values.\n",
    "\n",
    "Certain models, such as multi-task\n",
    "learning models, require the use of multiple loss functions simultaneously. In\n",
    "other words, the loss function ultimately being minimized is the sum of various\n",
    "other loss functions. For example, consider a model that predicts both\n",
    "the type of scene in an image as well as the depth from the\n",
    "camera of each pixel. This model's loss function would be the sum of the\n",
    "classification loss and depth prediction loss.\n",
    "\n",
    "TF-Slim provides an easy-to-use mechanism for defining and keeping track of\n",
    "loss functions via the\n",
    "[losses](https://www.tensorflow.org/code/tensorflow/contrib/losses/python/losses/loss_ops.py)\n",
    "module. Consider the simple case where we want to train the VGG network:\n",
    "\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "vgg = tf.contrib.slim.nets.vgg\n",
    "\n",
    "# Load the images and labels.\n",
    "images, labels = ...\n",
    "\n",
    "# Create the model.\n",
    "predictions, _ = vgg.vgg_16(images)\n",
    "\n",
    "# Define the loss functions and get the total loss.\n",
    "loss = slim.losses.softmax_cross_entropy(predictions, labels)\n",
    "```\n",
    "\n",
    "In this example, we start by creating the model (using TF-Slim's VGG\n",
    "implementation), and add the standard classification loss. Now, lets turn\n",
    "to the case where we have a multi-task model that produces multiple outputs:\n",
    "\n",
    "\n",
    "```python\n",
    "# Load the images and labels.\n",
    "images, scene_labels, depth_labels = ...\n",
    "\n",
    "# Create the model.\n",
    "scene_predictions, depth_predictions = CreateMultiTaskModel(images)\n",
    "\n",
    "# Define the loss functions and get the total loss.\n",
    "classification_loss = slim.losses.softmax_cross_entropy(scene_predictions, scene_labels)\n",
    "sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)\n",
    "\n",
    "# The following two lines have the same effect:\n",
    "total_loss = classification_loss + sum_of_squares_loss\n",
    "total_loss = slim.losses.get_total_loss(add_regularization_losses=False)\n",
    "```\n",
    "\n",
    "In this example, we have two losses which we add by calling\n",
    "`slim.losses.softmax_cross_entropy` and `slim.losses.sum_of_squares`. We can\n",
    "obtain the total loss by adding them together (`total_loss`) or by calling\n",
    "`slim.losses.get_total_loss()`. How did this work?\n",
    "When you create a loss function via TF-Slim, TF-Slim adds the loss to a\n",
    "special TensorFlow collection of loss functions. This enables you to either\n",
    "manage the total loss manually, or allow TF-Slim to manage them for you.\n",
    "\n",
    "What if you want to let TF-Slim manage the losses for you but have a custom loss\n",
    "function?\n",
    "[loss_ops.py](https://www.tensorflow.org/code/tensorflow/contrib/losses/python/losses/loss_ops.py)\n",
    "also has a function that adds this loss to TF-Slims collection. For example:\n",
    "\n",
    "```python\n",
    "# Load the images and labels.\n",
    "images, scene_labels, depth_labels, pose_labels = ...\n",
    "\n",
    "# Create the model.\n",
    "scene_predictions, depth_predictions, pose_predictions = CreateMultiTaskModel(images)\n",
    "\n",
    "# Define the loss functions and get the total loss.\n",
    "classification_loss = slim.losses.softmax_cross_entropy(scene_predictions, scene_labels)\n",
    "sum_of_squares_loss = slim.losses.sum_of_squares(depth_predictions, depth_labels)\n",
    "pose_loss = MyCustomLossFunction(pose_predictions, pose_labels)\n",
    "slim.losses.add_loss(pose_loss) # Letting TF-Slim know about the additional loss.\n",
    "\n",
    "# The following two ways to compute the total loss are equivalent:\n",
    "regularization_loss = tf.add_n(slim.losses.get_regularization_losses())\n",
    "total_loss1 = classification_loss + sum_of_squares_loss + pose_loss + regularization_loss\n",
    "\n",
    "# (Regularization Loss is included in the total loss by default).\n",
    "total_loss2 = slim.losses.get_total_loss()\n",
    "```\n",
    "In this example, we can again either produce the total loss function manually\n",
    "or let TF-Slim know about the additional loss and let TF-Slim handle the losses.\n",
    "\n",
    "### Training Loop\n",
    "\n",
    "TF-Slim provides a simple but powerful set of tools for training models\n",
    "found in\n",
    "[learning.py](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/learning.py).\n",
    "These include a Train function that repeatedly measures the loss, computes\n",
    "gradients and saves the model to disk, as well as several convenience functions\n",
    "for manipulating gradients. For example, once we've\n",
    "specified the model, the loss function and the optimization scheme, we can\n",
    "call `slim.learning.create_train_op` and `slim.learning.train` to perform the\n",
    "optimization:\n",
    "\n",
    "```python\n",
    "g = tf.Graph()\n",
    "\n",
    "# Create the model and specify the losses...\n",
    "...\n",
    "\n",
    "total_loss = slim.losses.get_total_loss()\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# create_train_op ensures that each time we ask for the loss, the update_ops\n",
    "# are run and the gradients being computed are applied too.\n",
    "train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "logdir = ... # Where checkpoints are stored.\n",
    "\n",
    "slim.learning.train(\n",
    "    train_op,\n",
    "    logdir,\n",
    "    number_of_steps=1000,\n",
    "    save_summaries_secs=300,\n",
    "    save_interval_secs=600):\n",
    "```\n",
    "\n",
    "In this example, `slim.learning.train` is provided with the `train_op` which is\n",
    "used to (a) compute the loss and (b) apply the gradient step. `logdir` specifies\n",
    "the directory where the checkpoints and event files are stored. We can limit the\n",
    "number of gradient steps taken to any number. In this case, we've asked for\n",
    "`1000` steps to be taken. Finally, `save_summaries_secs=300` indicates that\n",
    "we'll compute summaries every 5 minutes and `save_interval_secs=600` indicates\n",
    "that we'll save a model checkpoint every 10 minutes.\n",
    "\n",
    "### Working Example: Training the VGG16 Model\n",
    "\n",
    "To illustrate this, lets\n",
    "examine the following sample of training the VGG network:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "vgg = tf.contrib.slim.nets.vgg\n",
    "\n",
    "...\n",
    "\n",
    "train_log_dir = ...\n",
    "if not tf.gfile.Exists(train_log_dir):\n",
    "  tf.gfile.MakeDirs(train_log_dir)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  # Set up the data loading:\n",
    "  images, labels = ...\n",
    "\n",
    "  # Define the model:\n",
    "  predictions = vgg.vgg16(images, is_training=True)\n",
    "\n",
    "  # Specify the loss function:\n",
    "  slim.losses.softmax_cross_entropy(predictions, labels)\n",
    "\n",
    "  total_loss = slim.losses.get_total_loss()\n",
    "  tf.summary.scalar('losses/total_loss', total_loss)\n",
    "\n",
    "  # Specify the optimization scheme:\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate=.001)\n",
    "\n",
    "  # create_train_op that ensures that when we evaluate it to get the loss,\n",
    "  # the update_ops are done and the gradient updates are computed.\n",
    "  train_tensor = slim.learning.create_train_op(total_loss, optimizer)\n",
    "\n",
    "  # Actually runs training.\n",
    "  slim.learning.train(train_tensor, train_log_dir)\n",
    "```\n",
    "\n",
    "## Fine-Tuning Existing Models\n",
    "\n",
    "### Brief Recap on Restoring Variables from a Checkpoint\n",
    "\n",
    "After a model has been trained, it can be restored using `tf.train.Saver()`\n",
    "which restores `Variables` from a given checkpoint. For many cases,\n",
    "`tf.train.Saver()` provides a simple mechanism to restore all or just a\n",
    "few variables.\n",
    "\n",
    "```python\n",
    "# Create some variables.\n",
    "v1 = tf.Variable(..., name=\"v1\")\n",
    "v2 = tf.Variable(..., name=\"v2\")\n",
    "...\n",
    "# Add ops to restore all the variables.\n",
    "restorer = tf.train.Saver()\n",
    "\n",
    "# Add ops to restore some variables.\n",
    "restorer = tf.train.Saver([v1, v2])\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "  restorer.restore(sess, \"/tmp/model.ckpt\")\n",
    "  print(\"Model restored.\")\n",
    "  # Do some work with the model\n",
    "  ...\n",
    "```\n",
    "\n",
    "See [Restoring Variables](https://www.tensorflow.org/how_tos/variables/index.html#restoring-variables)\n",
    "and\n",
    "[Choosing which Variables to Save and Restore](https://www.tensorflow.org/how_tos/variables/index.html#choosing-which-variables-to-save-and-restore)\n",
    "sections of the [Variables](https://www.tensorflow.org/how_tos/variables/index.html)\n",
    "page for more details.\n",
    "\n",
    "### Partially Restoring Models\n",
    "\n",
    "It is often desirable to fine-tune a pre-trained model on an entirely new\n",
    "dataset or even a new task. In these situations, one can use TF-Slim's\n",
    "helper functions to select a subset of variables to restore:\n",
    "\n",
    "```python\n",
    "# Create some variables.\n",
    "v1 = slim.variable(name=\"v1\", ...)\n",
    "v2 = slim.variable(name=\"nested/v2\", ...)\n",
    "...\n",
    "\n",
    "# Get list of variables to restore (which contains only 'v2'). These are all\n",
    "# equivalent methods:\n",
    "variables_to_restore = slim.get_variables_by_name(\"v2\")\n",
    "# or\n",
    "variables_to_restore = slim.get_variables_by_suffix(\"2\")\n",
    "# or\n",
    "variables_to_restore = slim.get_variables(scope=\"nested\")\n",
    "# or\n",
    "variables_to_restore = slim.get_variables_to_restore(include=[\"nested\"])\n",
    "# or\n",
    "variables_to_restore = slim.get_variables_to_restore(exclude=[\"v1\"])\n",
    "\n",
    "# Create the saver which will be used to restore the variables.\n",
    "restorer = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "  restorer.restore(sess, \"/tmp/model.ckpt\")\n",
    "  print(\"Model restored.\")\n",
    "  # Do some work with the model\n",
    "  ...\n",
    "```\n",
    "\n",
    "### Restoring models with different variable names\n",
    "\n",
    "When restoring variables from a checkpoint, the `Saver`\n",
    "locates the variable names in a checkpoint file and maps them to variables in\n",
    "the current graph. Above, we created a saver by passing to it a list of\n",
    "variables. In this case, the names of the variables to locate in the checkpoint\n",
    "file were implicitly obtained from each provided variable's `var.op.name`.\n",
    "\n",
    "This works well when the variable names in the checkpoint file match those in\n",
    "the graph. However, sometimes, we want to restore a model from a checkpoint\n",
    "whose variables have different names those in the current graph. In this case,\n",
    "we must provide the `Saver` a dictionary that maps from each checkpoint variable\n",
    "name to each graph variable. Consider the following example where the checkpoint\n",
    "variables names are obtained via a simple function:\n",
    "\n",
    "```python\n",
    "# Assuming than 'conv1/weights' should be restored from 'vgg16/conv1/weights'\n",
    "def name_in_checkpoint(var):\n",
    "  return 'vgg16/' + var.op.name\n",
    "\n",
    "# Assuming than 'conv1/weights' and 'conv1/bias' should be restored from 'conv1/params1' and 'conv1/params2'\n",
    "def name_in_checkpoint(var):\n",
    "  if \"weights\" in var.op.name:\n",
    "    return var.op.name.replace(\"weights\", \"params1\")\n",
    "  if \"bias\" in var.op.name:\n",
    "    return var.op.name.replace(\"bias\", \"params2\")\n",
    "\n",
    "variables_to_restore = slim.get_model_variables()\n",
    "variables_to_restore = {name_in_checkpoint(var):var for var in variables_to_restore}\n",
    "restorer = tf.train.Saver(variables_to_restore)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  # Restore variables from disk.\n",
    "  restorer.restore(sess, \"/tmp/model.ckpt\")\n",
    "```\n",
    "\n",
    "### Fine-Tuning a Model on a different task\n",
    "\n",
    "Consider the case where we have a pre-trained VGG16 model. The model was\n",
    "trained on the ImageNet dataset, which has 1000 classes. However, we would\n",
    "like to apply it to the Pascal VOC dataset which has only 20 classes. To\n",
    "do so, we can initialize our new model using the values of the pre-trained\n",
    "model excluding the final layer:\n",
    "\n",
    "```python\n",
    "# Load the Pascal VOC data\n",
    "image, label = MyPascalVocDataLoader(...)\n",
    "images, labels = tf.train.batch([image, label], batch_size=32)\n",
    "\n",
    "# Create the model\n",
    "predictions = vgg.vgg_16(images)\n",
    "\n",
    "train_op = slim.learning.create_train_op(...)\n",
    "\n",
    "# Specify where the Model, trained on ImageNet, was saved.\n",
    "model_path = '/path/to/pre_trained_on_imagenet.checkpoint'\n",
    "\n",
    "# Specify where the new model will live:\n",
    "log_dir = '/path/to/my_pascal_model_dir/'\n",
    "\n",
    "# Restore only the convolutional layers:\n",
    "variables_to_restore = slim.get_variables_to_restore(exclude=['fc6', 'fc7', 'fc8'])\n",
    "init_fn = assign_from_checkpoint_fn(model_path, variables_to_restore)\n",
    "\n",
    "# Start training.\n",
    "slim.learning.train(train_op, log_dir, init_fn=init_fn)\n",
    "```\n",
    "\n",
    "## Evaluating Models.\n",
    "\n",
    "Once we've trained a model (or even while the model is busy training) we'd like\n",
    "to see how well the model performs in practice. This is accomplished by picking\n",
    "a set of evaluation metrics, which will grade the models performance, and the\n",
    "evaluation code which actually loads the data, performs inference, compares the\n",
    "results to the ground truth and records the evaluation scores. This step may be\n",
    "performed once or repeated periodically.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "We define a metric to be a performance measure that is not a loss function\n",
    "(losses are directly optimized during training), but which we are still\n",
    "interested in for the purpose of evaluating our model.\n",
    "For example, we might want to minimize log loss, but our metrics of interest\n",
    "might be F1 score (test accuracy), or Intersection Over Union score (which are not\n",
    "differentiable, and therefore cannot be used as losses).\n",
    "\n",
    "TF-Slim provides a set of metric operations that makes evaluating models\n",
    "easy. Abstractly, computing the value of a metric can be divided into three\n",
    "parts:\n",
    "\n",
    "1. Initialization: initialize the variables used to compute the metrics.\n",
    "2. Aggregation: perform operations (sums, etc) used to compute the metrics.\n",
    "3. Finalization: (optionally) perform any final operation to compute metric\n",
    "values. For example, computing means, mins, maxes, etc.\n",
    "\n",
    "For example, to compute `mean_absolute_error`, two variables, a `count` and\n",
    "`total` variable are *initialized* to zero. During *aggregation*, we observed\n",
    "some set of predictions and labels, compute their absolute differences and add\n",
    "the total to `total`. Each time we observe another value,\n",
    "`count` is incremented. Finally, during *finalization*, `total` is divided\n",
    "by `count` to obtain the mean.\n",
    "\n",
    "The following example demonstrates the API for declaring metrics. Because\n",
    "metrics are often evaluated on a test set which is different from the training\n",
    "set (upon which the loss is computed), we'll assume we're using test data:\n",
    "\n",
    "```python\n",
    "images, labels = LoadTestData(...)\n",
    "predictions = MyModel(images)\n",
    "\n",
    "mae_value_op, mae_update_op = slim.metrics.streaming_mean_absolute_error(predictions, labels)\n",
    "mre_value_op, mre_update_op = slim.metrics.streaming_mean_relative_error(predictions, labels)\n",
    "pl_value_op, pl_update_op = slim.metrics.percentage_less(mean_relative_errors, 0.3)\n",
    "```\n",
    "\n",
    "As the example illustrates, the creation of a metric returns two values:\n",
    "a *value_op* and an *update_op*. The value_op is an idempotent operation that\n",
    "returns the current value of the metric. The update_op is an operation that\n",
    "performs the *aggregation* step mentioned above as well as returning the value\n",
    "of the metric.\n",
    "\n",
    "Keeping track of each `value_op` and `update_op` can be laborious. To deal with\n",
    "this, TF-Slim provides two convenience functions:\n",
    "\n",
    "```python\n",
    "\n",
    "# Aggregates the value and update ops in two lists:\n",
    "value_ops, update_ops = slim.metrics.aggregate_metrics(\n",
    "    slim.metrics.streaming_mean_absolute_error(predictions, labels),\n",
    "    slim.metrics.streaming_mean_squared_error(predictions, labels))\n",
    "\n",
    "# Aggregates the value and update ops in two dictionaries:\n",
    "names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "    \"eval/mean_absolute_error\": slim.metrics.streaming_mean_absolute_error(predictions, labels),\n",
    "    \"eval/mean_squared_error\": slim.metrics.streaming_mean_squared_error(predictions, labels),\n",
    "})\n",
    "\n",
    "```\n",
    "\n",
    "### Working example: Tracking Multiple Metrics\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "vgg = tf.contrib.slim.nets.vgg\n",
    "\n",
    "\n",
    "# Load the data\n",
    "images, labels = load_data(...)\n",
    "\n",
    "# Define the network\n",
    "predictions = vgg.vgg_16(images)\n",
    "\n",
    "# Choose the metrics to compute:\n",
    "names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "    \"eval/mean_absolute_error\": slim.metrics.streaming_mean_absolute_error(predictions, labels),\n",
    "    \"eval/mean_squared_error\": slim.metrics.streaming_mean_squared_error(predictions, labels),\n",
    "})\n",
    "\n",
    "# Evaluate the model using 1000 batches of data:\n",
    "num_batches = 1000\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  sess.run(tf.local_variables_initializer())\n",
    "\n",
    "  for batch_id in range(num_batches):\n",
    "    sess.run(names_to_updates.values())\n",
    "\n",
    "  metric_values = sess.run(names_to_values.values())\n",
    "  for metric, value in zip(names_to_values.keys(), metric_values):\n",
    "    print('Metric %s has value: %f' % (metric, value))\n",
    "```\n",
    "\n",
    "Note that\n",
    "[metric_ops.py](https://www.tensorflow.org/code/tensorflow/contrib/metrics/python/ops/metric_ops.py)\n",
    "can be used in isolation without using either\n",
    "[layers.py](https://www.tensorflow.org/code/tensorflow/contrib/layers/python/layers/layers.py)\n",
    "or\n",
    "[loss_ops.py](https://www.tensorflow.org/code/tensorflow/contrib/losses/python/losses/loss_ops.py)\n",
    "\n",
    "### Evaluation Loop\n",
    "\n",
    "TF-Slim provides an evaluation module\n",
    "([evaluation.py](https://www.tensorflow.org/code/tensorflow/contrib/slim/python/slim/evaluation.py)),\n",
    "which contains helper functions for writing model evaluation scripts using\n",
    "metrics from\n",
    "the [metric_ops.py](https://www.tensorflow.org/code/tensorflow/contrib/metrics/python/ops/metric_ops.py)\n",
    "module. These include a function for periodically running evaluations,\n",
    "evaluating\n",
    "metrics over batches of data and printing and summarizing metric results. For\n",
    "example:\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "\n",
    "# Load the data\n",
    "images, labels = load_data(...)\n",
    "\n",
    "# Define the network\n",
    "predictions = MyModel(images)\n",
    "\n",
    "# Choose the metrics to compute:\n",
    "names_to_values, names_to_updates = slim.metrics.aggregate_metric_map({\n",
    "    'accuracy': slim.metrics.accuracy(predictions, labels),\n",
    "    'precision': slim.metrics.precision(predictions, labels),\n",
    "    'recall': slim.metrics.recall(mean_relative_errors, 0.3),\n",
    "})\n",
    "\n",
    "# Create the summary ops such that they also print out to std output:\n",
    "summary_ops = []\n",
    "for metric_name, metric_value in names_to_values.iteritems():\n",
    "  op = tf.summary.scalar(metric_name, metric_value)\n",
    "  op = tf.Print(op, [metric_value], metric_name)\n",
    "  summary_ops.append(op)\n",
    "\n",
    "num_examples = 10000\n",
    "batch_size = 32\n",
    "num_batches = math.ceil(num_examples / float(batch_size))\n",
    "\n",
    "# Setup the global step.\n",
    "slim.get_or_create_global_step()\n",
    "\n",
    "output_dir = ... # Where the summaries are stored.\n",
    "eval_interval_secs = ... # How often to run the evaluation.\n",
    "slim.evaluation.evaluation_loop(\n",
    "    'local',\n",
    "    checkpoint_dir,\n",
    "    log_dir,\n",
    "    num_evals=num_batches,\n",
    "    eval_op=names_to_updates.values(),\n",
    "    summary_op=tf.summary.merge(summary_ops),\n",
    "    eval_interval_secs=eval_interval_secs)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
