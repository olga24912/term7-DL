{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'CPUOptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bb3267d420a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mgpu_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGPUOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_process_gpu_memory_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.333\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mcpu_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCPUOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mper_process_cpu_memory_fraction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.333\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'CPUOptions'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adagrad\n",
    "\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "cpu_options = tf.CPUOptions(per_process_cpu_memory_fraction=0.333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: fill empty spaces in the following agent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DeepQAgent:\n",
    "    def __init__(self, state_size, action_size, render=True):\n",
    "        # Tip: if you are training this on AWS the best way is to turn off rendering\n",
    "        # and load it later with the serialized model\n",
    "        self.render = render\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.discount_factor = 0.99\n",
    "        self.learning_rate = 0.001\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.005\n",
    "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / 50000\n",
    "        self.batch_size = 64\n",
    "        self.train_start = 1000\n",
    "        # replay memory\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, activation='relu', input_dim = self.state_size))\n",
    "        model.add(Dense(self.action_size, activation='softmax'))\n",
    "        \n",
    "        adagrad = Adagrad(lr=self.learning_rate)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adagrad,\n",
    "              metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        print(\"update\")\n",
    "        \"\"\"Update your target model to the model you are currently learning at regular time intervals\"\"\"        \n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"The choice of action uses the epsilon-greedy policy for the current network.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            if (state[0][0] < -0.6 and state[0][1] < -0.01):\n",
    "                return 0\n",
    "            elif (state[0][0] > -0.4 and state[0][1] > 0.01):\n",
    "                return 1\n",
    "            elif (state[0][0] < -0.6 and state[0][1] > -0.01):\n",
    "                return 1\n",
    "            elif (state[0][0] > -0.4 and state[0][1] < 0.01):\n",
    "                return 0\n",
    "            elif (state[0][0] > -0.6 and state[0][0] < -0.4 and state[0][1] < 0):\n",
    "                return 0\n",
    "            elif (state[0][0] > -0.6 and state[0][0] < -0.4 and state[0][1] > 0):\n",
    "                return 1\n",
    "            \n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(state)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    def replay_memory(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save <s, a, r, s'> to replay_memory\"\"\"\n",
    "        if action == 2:\n",
    "            action = 1\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "            # print(len(self.memory))\n",
    "\n",
    "    def train_replay(self):\n",
    "        \"\"\"Random sampling of batch_size samples from replay memory\"\"\"\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        batch_size = min(self.batch_size, len(self.memory))\n",
    "        mini_batch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        update_input = np.zeros((batch_size, self.state_size))\n",
    "        update_target = np.zeros((batch_size, self.action_size))\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, done = mini_batch[i]\n",
    "            target = self.model.predict(state)[0]\n",
    "\n",
    "            # As in queuing, it gets the maximum Q Value at s'. However, it is imported from the target model.\n",
    "            if done:\n",
    "                target[action] = reward\n",
    "            else:\n",
    "                target[action] = reward + self.discount_factor * \\\n",
    "                                          np.amax(self.target_model.predict(next_state)[0])\n",
    "            update_input[i] = state\n",
    "            update_target[i] = target\n",
    "\n",
    "        # You can create a minibatch of the correct target answer and the current value of your own,\n",
    "        self.model.fit(update_input, update_target, batch_size=batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.model.load_model(name)\n",
    "\n",
    "    def save_model(self, name):\n",
    "        self.model.save(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "update\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "state_size = env.observation_space.shape[0] # should be equal 2\n",
    "ACTION_SIZE = 2\n",
    "agent = DeepQAgent(state_size, ACTION_SIZE)\n",
    "#agent.load_model(\"./save_model/model\")\n",
    "scores, episodes = [], []\n",
    "N_EPISODES = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state [[-0.42618867  0.        ]]\n",
      "[[-0.43644821 -0.00511124]]\n",
      "[[-0.47280497 -0.01135408]]\n",
      "[[-0.53102032 -0.01626486]]\n",
      "[[-0.60420776 -0.01924799]]\n",
      "[[-0.68379855 -0.02000606]]\n",
      "[[-0.76099877 -0.01862991]]\n",
      "[[-0.82822767 -0.01553181]]\n",
      "[[-0.87995059 -0.01125657]]\n",
      "[[-0.91273456 -0.00630067]]\n",
      "[[-0.90490461  0.00691058]]\n",
      "[[-0.84490299  0.01976031]]\n",
      "[[-0.73647251  0.03126155]]\n",
      "[[-0.58877065  0.03984665]]\n",
      "[[-0.41764795  0.04390838]]\n",
      "[[-0.24278938  0.04302823]]\n",
      "[[-0.08110006  0.03854208]]\n",
      "[[ 0.05843703  0.03264571]]\n",
      "[[ 0.17500176  0.02717435]]\n",
      "[[ 0.27314496  0.02317539]]\n",
      "[[ 0.36006784  0.02112488]]\n",
      "[[ 0.44421632  0.02127402]]\n",
      "update\n",
      "episode: 0   score: -86.0   memory length: 86   epsilon: 0.9999828859999971\n",
      "state [[-0.59343473  0.        ]]\n",
      "[[-0.59629747 -0.00142611]]\n",
      "[[-0.60643864 -0.00316683]]\n",
      "[[-0.60290303  0.00331326]]\n",
      "[[-0.57417347  0.00940937]]\n",
      "[[-0.52360838  0.01440049]]\n",
      "[[-0.45719085  0.01768383]]\n",
      "[[-0.38273489  0.01890556]]\n",
      "[[-0.30861564  0.0180595 ]]\n",
      "[[-0.24239832  0.01546078]]\n",
      "[[-0.18990139  0.01159607]]\n",
      "[[-0.15495143  0.0069519 ]]\n",
      "[[-0.15956059 -0.00601794]]\n",
      "[[-0.21542311 -0.01863962]]\n",
      "[[-0.31880311 -0.02991677]]\n",
      "[[-0.46070081 -0.03834661]]\n",
      "[[-0.62576767 -0.04240849]]\n",
      "[[-0.79497223 -0.04167949]]\n",
      "[[-0.95172024 -0.03736729]]\n",
      "[[-1.08673081 -0.03151663]]\n",
      "[[-1.19842025 -0.02587075]]\n",
      "[[-1.19252817  0.00374203]]\n",
      "[[-1.14464216  0.01696802]]\n",
      "[[-1.04246536  0.03075592]]\n",
      "[[-0.88478057  0.04451789]]\n",
      "[[-0.67680136  0.05598691]]\n",
      "[[-0.43597114  0.06187089]]\n",
      "[[-0.18914453  0.06082057]]\n",
      "[[ 0.0413786   0.05545726]]\n",
      "[[ 0.24950724  0.05025378]]\n",
      "[[ 0.44506688  0.04867242]]\n",
      "update\n",
      "episode: 1   score: -121.0   memory length: 207   epsilon: 0.999958806999993\n",
      "state [[-0.5675144  0.       ]]\n",
      "[[-0.57151899 -0.00199483]]\n",
      "[[-0.58569833 -0.00442668]]\n",
      "[[-0.60839173 -0.00634139]]\n",
      "[[ -6.17185449e-01   3.31346782e-04]]\n",
      "[[-0.59913965  0.00696603]]\n",
      "[[-0.55633992  0.01279273]]\n",
      "[[-0.49380994  0.01710946]]\n",
      "[[-0.41895254  0.01940435]]\n",
      "[[-0.34042744  0.01949614]]\n",
      "[[-0.2666679   0.01758001]]\n",
      "[[-0.20460551  0.01411757]]\n",
      "[[-0.15908384  0.00964291]]\n",
      "[[-0.15288006 -0.00332548]]\n",
      "[[-0.19831176 -0.0161069 ]]\n",
      "[[-0.29249539 -0.02779739]]\n",
      "[[-0.42770749 -0.03698816]]\n",
      "[[-0.58985261 -0.04207107]]\n",
      "[[-0.76018437 -0.04230135]]\n",
      "[[-0.92102584 -0.03857453]]\n",
      "[[-1.06136431 -0.03287549]]\n",
      "[[-1.17819322 -0.0270853 ]]\n",
      "[[-1.19252817  0.00374203]]\n",
      "[[-1.14464216  0.01696802]]\n",
      "[[-1.04246536  0.03075592]]\n",
      "[[-0.88478057  0.04451789]]\n",
      "[[-0.67680136  0.05598691]]\n",
      "[[-0.43597114  0.06187089]]\n",
      "[[-0.18914453  0.06082057]]\n",
      "[[ 0.0413786   0.05545726]]\n",
      "[[ 0.24950724  0.05025378]]\n",
      "[[ 0.44506688  0.04867242]]\n",
      "update\n",
      "episode: 2   score: -125.0   memory length: 332   epsilon: 0.9999339319999887\n",
      "state [[-0.58356163  0.        ]]\n",
      "[[-0.58685747 -0.00164182]]\n",
      "[[-0.59853049 -0.00364478]]\n",
      "[[-0.61722423 -0.00522528]]\n",
      "[[-0.62100328  0.00164957]]\n",
      "[[-0.59750711  0.00833566]]\n",
      "[[-0.54944848  0.01405454]]\n",
      "[[-0.48247543  0.01811233]]\n",
      "[[-0.40450794  0.02003657]]\n",
      "[[-0.3244841   0.01971606]]\n",
      "[[-0.25081336  0.0174205 ]]\n",
      "[[-0.19016552  0.01365889]]\n",
      "[[-0.14702896  0.00897461]]\n",
      "[[-0.14386527 -0.00413447]]\n",
      "[[-0.19280896 -0.01702332]]\n",
      "[[-0.29086084 -0.02878894]]\n",
      "[[-0.43007413 -0.03798108]]\n",
      "[[-0.59594534 -0.0429511 ]]\n",
      "[[-0.76931545 -0.04298349]]\n",
      "[[-0.93241025 -0.03907578]]\n",
      "[[-1.07453779 -0.03330815]]\n",
      "[[-1.19325724 -0.02760164]]\n",
      "[[-1.19252817  0.00374203]]\n",
      "[[-1.14464216  0.01696802]]\n",
      "[[-1.04246536  0.03075592]]\n",
      "[[-0.88478057  0.04451789]]\n",
      "[[-0.67680136  0.05598691]]\n",
      "[[-0.43597114  0.06187089]]\n",
      "[[-0.18914453  0.06082057]]\n",
      "[[ 0.0413786   0.05545726]]\n",
      "[[ 0.24950724  0.05025378]]\n",
      "[[ 0.44506688  0.04867242]]\n",
      "update\n",
      "episode: 3   score: -125.0   memory length: 457   epsilon: 0.9999090569999844\n",
      "state [[-0.53535195  0.        ]]\n",
      "[[-0.54078904 -0.00270832]]\n",
      "[[-0.56003441 -0.00600711]]\n",
      "[[-0.59081019 -0.00859626]]\n",
      "[[-0.6295091  -0.01018347]]\n",
      "[[-0.6716952  -0.01061693]]\n",
      "[[-0.71272003 -0.00990257]]\n",
      "[[ -7.28497270e-01  -3.05122042e-04]]\n",
      "[[-0.70550701  0.00932257]]\n",
      "[[-0.64607526  0.01799586]]\n",
      "[[-0.55659526  0.02468267]]\n",
      "[[-0.44737278  0.02847656]]\n",
      "[[-0.33119717  0.02897714]]\n",
      "[[-0.22060382  0.02653124]]\n",
      "[[-0.1252394   0.02203524]]\n",
      "[[-0.05082192  0.01646353]]\n",
      "[[ 0.00020389  0.01051376]]\n",
      "[[ 0.02727391  0.00452368]]\n",
      "[[ 0.0104486  -0.00944698]]\n",
      "[[-0.06230913 -0.02342466]]\n",
      "[[-0.18998974 -0.03687411]]\n",
      "[[-0.36654932 -0.04805828]]\n",
      "[[-0.57651541 -0.05440167]]\n",
      "[[-0.79628277 -0.05451393]]\n",
      "[[-1.00387571 -0.04994319]]\n",
      "[[-1.18890109 -0.04410146]]\n",
      "[[-1.19252817  0.00374203]]\n",
      "[[-1.14464216  0.01696802]]\n",
      "[[-1.04246536  0.03075592]]\n",
      "[[-0.88478057  0.04451789]]\n",
      "[[-0.67680136  0.05598691]]\n",
      "[[-0.43597114  0.06187089]]\n",
      "[[-0.18914453  0.06082057]]\n",
      "[[ 0.0413786   0.05545726]]\n",
      "[[ 0.24950724  0.05025378]]\n",
      "[[ 0.44506688  0.04867242]]\n",
      "update\n",
      "episode: 4   score: -141.0   memory length: 598   epsilon: 0.9998809979999796\n",
      "state [[-0.46431843  0.        ]]\n",
      "[[-0.47291881 -0.00428424]]\n",
      "[[-0.50337186 -0.0095067 ]]\n",
      "[[-0.55207642 -0.01360233]]\n",
      "[[-0.61325862 -0.01608875]]\n",
      "[[-0.67979398 -0.01672655]]\n",
      "[[-0.74433706 -0.01557168]]\n",
      "[[-0.80041263 -0.01292611]]\n",
      "[[-0.84309578 -0.0092111 ]]\n",
      "[[-0.84928601  0.00307664]]\n",
      "[[-0.80659209  0.01515   ]]\n",
      "[[-0.71825228  0.02601954]]\n",
      "[[-0.59233318  0.03434416]]\n",
      "[[-0.44255023  0.03875418]]\n",
      "[[-0.28629209  0.0387164 ]]\n",
      "[[-0.13967036  0.03506472]]\n",
      "[[-0.01302985  0.02948688]]\n",
      "[[ 0.09002101  0.02356014]]\n",
      "[[ 0.17069416  0.01824418]]\n",
      "[[ 0.23253596  0.01392225]]\n",
      "[[ 0.279698    0.01063623]]\n",
      "[[ 0.31609222  0.00828436]]\n",
      "[[ 0.32493635 -0.00139092]]\n",
      "[[ 0.29503229 -0.01120297]]\n",
      "[[ 0.22354808 -0.02205104]]\n",
      "[[ 0.10465565 -0.03454444]]\n",
      "[[-0.06786474 -0.04836079]]\n",
      "[[-0.29436416 -0.06122678]]\n",
      "[[-0.56089647 -0.0689759 ]]\n",
      "[[-0.83910146 -0.0689576 ]]\n",
      "[[-1.10259331 -0.06370828]]\n",
      "[[-1.19627021  0.0024879 ]]\n",
      "[[-1.15355085  0.01564864]]\n",
      "[[-1.05682847  0.02937283]]\n",
      "[[-0.90454967  0.04321545]]\n",
      "[[-0.7008884   0.05508905]]\n",
      "[[-0.46191467  0.06168987]]\n",
      "[[-0.21406428  0.06130218]]\n",
      "[[ 0.01913637  0.05617716]]\n",
      "[[ 0.22975533  0.05077358]]\n",
      "[[ 0.42630049  0.04873346]]\n",
      "update\n",
      "episode: 5   score: -161.0   memory length: 759   epsilon: 0.9998489589999742\n",
      "state [[-0.50704982  0.        ]]\n",
      "[[-0.51375217 -0.00333857]]\n",
      "[[-0.53747527 -0.00740449]]\n",
      "[[-0.57540116 -0.01059146]]\n",
      "[[-0.62305364 -0.01253434]]\n",
      "[[-0.674931   -0.01304846]]\n",
      "[[-0.72531172 -0.01215659]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1446f5e6fae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Select an action in the current state and proceed to a step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olga/soft/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olga/soft/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olga/soft/anaconda3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unsupported rendering mode: {}. (Supported modes for {}: {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olga/soft/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/mountain_car.py\u001b[0m in \u001b[0;36m_render\u001b[0;34m(self, mode, close)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcartrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_rotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/olga/soft/anaconda3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mglClearColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/olga/soft/anaconda3/lib/python3.6/site-packages/pyglet/gl/lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[0;34m(result, func, arguments)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mGLException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No GL context; create a Window first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gl_begin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglGetError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgluErrorString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for e in range(N_EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    print(\"state\", state)\n",
    "\n",
    "    # Action 0 (left), 1 (do nothing), 3 (declare fake_action to avoid doing nothing\n",
    "    fake_action = 0\n",
    "\n",
    "    # Counter for the same action 4 times\n",
    "    action_count = 0\n",
    "\n",
    "    while not done:\n",
    "        if agent.render:\n",
    "            env.render()\n",
    "\n",
    "        # Select an action in the current state and proceed to a step\n",
    "        action_count = action_count + 1\n",
    "\n",
    "        if action_count == 4:\n",
    "            action = agent.get_action(state)\n",
    "            action_count = 0\n",
    "\n",
    "            if action == 0:\n",
    "                fake_action = 0\n",
    "            elif action == 1:\n",
    "                fake_action = 2\n",
    "\n",
    "        # Take 1 step with the selected action\n",
    "        next_state, reward, done, info = env.step(fake_action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        # Give a penalty of -100 for actions that end an episode\n",
    "        # reward = reward if not done else -100\n",
    "\n",
    "        # Save <s, a, r, s'> to replay memory\n",
    "        agent.replay_memory(state, fake_action, reward, next_state, done)\n",
    "        # Continue to learn every time step\n",
    "        agent.train_replay()\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            env.reset()\n",
    "            # Copy the learning model for each episode to the target model\n",
    "            agent.update_target_model()\n",
    "\n",
    "            # For each episode, the time step where cartpole stood is plot\n",
    "            scores.append(score)\n",
    "            episodes.append(e)\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\", len(agent.memory),\n",
    "                  \"  epsilon:\", agent.epsilon)\n",
    "\n",
    "    # Save model for every 50 episodes\n",
    "    if e % 50 == 0:\n",
    "        agent.save_model(\"./save_model/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
